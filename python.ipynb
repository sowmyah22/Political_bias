{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPU8dFCw-d8d",
        "outputId": "b8e0a035-f027-4617-e530-ba70701bf58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcFcQxaM_YAQ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "from functools import lru_cache\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIjfBD9SCVo3",
        "outputId": "dc484d2b-ee10-46ec-e7f7-03b5b1f05b78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6FGZz1VJCP1-",
        "outputId": "2c4e1a3f-2dd5-40fe-fd58-c1110125a4c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1901' max='34260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1901/34260 20:51 < 5:55:21, 1.52 it/s, Epoch 0.55/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Left</th>\n",
              "      <th>F1 Center</th>\n",
              "      <th>F1 Right</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.029300</td>\n",
              "      <td>0.973672</td>\n",
              "      <td>0.481822</td>\n",
              "      <td>0.378991</td>\n",
              "      <td>0.510414</td>\n",
              "      <td>0.481822</td>\n",
              "      <td>0.006098</td>\n",
              "      <td>0.638540</td>\n",
              "      <td>0.492334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.909900</td>\n",
              "      <td>0.842196</td>\n",
              "      <td>0.573776</td>\n",
              "      <td>0.532541</td>\n",
              "      <td>0.557697</td>\n",
              "      <td>0.573776</td>\n",
              "      <td>0.236244</td>\n",
              "      <td>0.753892</td>\n",
              "      <td>0.607487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.822900</td>\n",
              "      <td>0.746254</td>\n",
              "      <td>0.631928</td>\n",
              "      <td>0.583247</td>\n",
              "      <td>0.642335</td>\n",
              "      <td>0.631928</td>\n",
              "      <td>0.280103</td>\n",
              "      <td>0.807600</td>\n",
              "      <td>0.662037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.861300</td>\n",
              "      <td>0.678825</td>\n",
              "      <td>0.713240</td>\n",
              "      <td>0.705823</td>\n",
              "      <td>0.715577</td>\n",
              "      <td>0.713240</td>\n",
              "      <td>0.568060</td>\n",
              "      <td>0.845971</td>\n",
              "      <td>0.703439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.661100</td>\n",
              "      <td>0.623676</td>\n",
              "      <td>0.741677</td>\n",
              "      <td>0.731955</td>\n",
              "      <td>0.741323</td>\n",
              "      <td>0.741677</td>\n",
              "      <td>0.625165</td>\n",
              "      <td>0.835651</td>\n",
              "      <td>0.735049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.527100</td>\n",
              "      <td>0.582055</td>\n",
              "      <td>0.781609</td>\n",
              "      <td>0.775785</td>\n",
              "      <td>0.782022</td>\n",
              "      <td>0.781609</td>\n",
              "      <td>0.692275</td>\n",
              "      <td>0.858733</td>\n",
              "      <td>0.776346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.541000</td>\n",
              "      <td>0.550824</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.771011</td>\n",
              "      <td>0.802817</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.648311</td>\n",
              "      <td>0.832081</td>\n",
              "      <td>0.832640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.391800</td>\n",
              "      <td>0.356931</td>\n",
              "      <td>0.865475</td>\n",
              "      <td>0.863388</td>\n",
              "      <td>0.866439</td>\n",
              "      <td>0.865475</td>\n",
              "      <td>0.799172</td>\n",
              "      <td>0.935624</td>\n",
              "      <td>0.855369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.356000</td>\n",
              "      <td>0.340595</td>\n",
              "      <td>0.884632</td>\n",
              "      <td>0.883449</td>\n",
              "      <td>0.885966</td>\n",
              "      <td>0.884632</td>\n",
              "      <td>0.840873</td>\n",
              "      <td>0.921381</td>\n",
              "      <td>0.888093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.380500</td>\n",
              "      <td>0.281326</td>\n",
              "      <td>0.902597</td>\n",
              "      <td>0.901385</td>\n",
              "      <td>0.903240</td>\n",
              "      <td>0.902597</td>\n",
              "      <td>0.861767</td>\n",
              "      <td>0.933252</td>\n",
              "      <td>0.909137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.284800</td>\n",
              "      <td>0.213355</td>\n",
              "      <td>0.929842</td>\n",
              "      <td>0.929314</td>\n",
              "      <td>0.930522</td>\n",
              "      <td>0.929842</td>\n",
              "      <td>0.904095</td>\n",
              "      <td>0.958829</td>\n",
              "      <td>0.925019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.208072</td>\n",
              "      <td>0.935717</td>\n",
              "      <td>0.935408</td>\n",
              "      <td>0.935821</td>\n",
              "      <td>0.935717</td>\n",
              "      <td>0.912818</td>\n",
              "      <td>0.956641</td>\n",
              "      <td>0.936765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.254800</td>\n",
              "      <td>0.220175</td>\n",
              "      <td>0.934185</td>\n",
              "      <td>0.933290</td>\n",
              "      <td>0.937179</td>\n",
              "      <td>0.934185</td>\n",
              "      <td>0.907248</td>\n",
              "      <td>0.947599</td>\n",
              "      <td>0.945022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.275800</td>\n",
              "      <td>0.177723</td>\n",
              "      <td>0.946786</td>\n",
              "      <td>0.946561</td>\n",
              "      <td>0.946933</td>\n",
              "      <td>0.946786</td>\n",
              "      <td>0.926421</td>\n",
              "      <td>0.964783</td>\n",
              "      <td>0.948480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.232800</td>\n",
              "      <td>0.163707</td>\n",
              "      <td>0.953086</td>\n",
              "      <td>0.952963</td>\n",
              "      <td>0.953510</td>\n",
              "      <td>0.953086</td>\n",
              "      <td>0.936977</td>\n",
              "      <td>0.969233</td>\n",
              "      <td>0.952678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.235200</td>\n",
              "      <td>0.134407</td>\n",
              "      <td>0.959813</td>\n",
              "      <td>0.959677</td>\n",
              "      <td>0.960222</td>\n",
              "      <td>0.959813</td>\n",
              "      <td>0.947437</td>\n",
              "      <td>0.971472</td>\n",
              "      <td>0.960120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.145700</td>\n",
              "      <td>0.182042</td>\n",
              "      <td>0.953768</td>\n",
              "      <td>0.953383</td>\n",
              "      <td>0.955229</td>\n",
              "      <td>0.953768</td>\n",
              "      <td>0.937441</td>\n",
              "      <td>0.963954</td>\n",
              "      <td>0.958753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.256800</td>\n",
              "      <td>0.337103</td>\n",
              "      <td>0.919029</td>\n",
              "      <td>0.918983</td>\n",
              "      <td>0.927024</td>\n",
              "      <td>0.919029</td>\n",
              "      <td>0.891021</td>\n",
              "      <td>0.909940</td>\n",
              "      <td>0.955986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='305' max='368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [305/368 00:31 < 00:06, 9.53 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3400' max='34260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3400/34260 39:03 < 5:54:42, 1.45 it/s, Epoch 0/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Left</th>\n",
              "      <th>F1 Center</th>\n",
              "      <th>F1 Right</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.029300</td>\n",
              "      <td>0.973672</td>\n",
              "      <td>0.481822</td>\n",
              "      <td>0.378991</td>\n",
              "      <td>0.510414</td>\n",
              "      <td>0.481822</td>\n",
              "      <td>0.006098</td>\n",
              "      <td>0.638540</td>\n",
              "      <td>0.492334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.909900</td>\n",
              "      <td>0.842196</td>\n",
              "      <td>0.573776</td>\n",
              "      <td>0.532541</td>\n",
              "      <td>0.557697</td>\n",
              "      <td>0.573776</td>\n",
              "      <td>0.236244</td>\n",
              "      <td>0.753892</td>\n",
              "      <td>0.607487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.822900</td>\n",
              "      <td>0.746254</td>\n",
              "      <td>0.631928</td>\n",
              "      <td>0.583247</td>\n",
              "      <td>0.642335</td>\n",
              "      <td>0.631928</td>\n",
              "      <td>0.280103</td>\n",
              "      <td>0.807600</td>\n",
              "      <td>0.662037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.861300</td>\n",
              "      <td>0.678825</td>\n",
              "      <td>0.713240</td>\n",
              "      <td>0.705823</td>\n",
              "      <td>0.715577</td>\n",
              "      <td>0.713240</td>\n",
              "      <td>0.568060</td>\n",
              "      <td>0.845971</td>\n",
              "      <td>0.703439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.661100</td>\n",
              "      <td>0.623676</td>\n",
              "      <td>0.741677</td>\n",
              "      <td>0.731955</td>\n",
              "      <td>0.741323</td>\n",
              "      <td>0.741677</td>\n",
              "      <td>0.625165</td>\n",
              "      <td>0.835651</td>\n",
              "      <td>0.735049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.527100</td>\n",
              "      <td>0.582055</td>\n",
              "      <td>0.781609</td>\n",
              "      <td>0.775785</td>\n",
              "      <td>0.782022</td>\n",
              "      <td>0.781609</td>\n",
              "      <td>0.692275</td>\n",
              "      <td>0.858733</td>\n",
              "      <td>0.776346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.541000</td>\n",
              "      <td>0.550824</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.771011</td>\n",
              "      <td>0.802817</td>\n",
              "      <td>0.783397</td>\n",
              "      <td>0.648311</td>\n",
              "      <td>0.832081</td>\n",
              "      <td>0.832640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.391800</td>\n",
              "      <td>0.356931</td>\n",
              "      <td>0.865475</td>\n",
              "      <td>0.863388</td>\n",
              "      <td>0.866439</td>\n",
              "      <td>0.865475</td>\n",
              "      <td>0.799172</td>\n",
              "      <td>0.935624</td>\n",
              "      <td>0.855369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.356000</td>\n",
              "      <td>0.340595</td>\n",
              "      <td>0.884632</td>\n",
              "      <td>0.883449</td>\n",
              "      <td>0.885966</td>\n",
              "      <td>0.884632</td>\n",
              "      <td>0.840873</td>\n",
              "      <td>0.921381</td>\n",
              "      <td>0.888093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.380500</td>\n",
              "      <td>0.281326</td>\n",
              "      <td>0.902597</td>\n",
              "      <td>0.901385</td>\n",
              "      <td>0.903240</td>\n",
              "      <td>0.902597</td>\n",
              "      <td>0.861767</td>\n",
              "      <td>0.933252</td>\n",
              "      <td>0.909137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.284800</td>\n",
              "      <td>0.213355</td>\n",
              "      <td>0.929842</td>\n",
              "      <td>0.929314</td>\n",
              "      <td>0.930522</td>\n",
              "      <td>0.929842</td>\n",
              "      <td>0.904095</td>\n",
              "      <td>0.958829</td>\n",
              "      <td>0.925019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.208072</td>\n",
              "      <td>0.935717</td>\n",
              "      <td>0.935408</td>\n",
              "      <td>0.935821</td>\n",
              "      <td>0.935717</td>\n",
              "      <td>0.912818</td>\n",
              "      <td>0.956641</td>\n",
              "      <td>0.936765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.254800</td>\n",
              "      <td>0.220175</td>\n",
              "      <td>0.934185</td>\n",
              "      <td>0.933290</td>\n",
              "      <td>0.937179</td>\n",
              "      <td>0.934185</td>\n",
              "      <td>0.907248</td>\n",
              "      <td>0.947599</td>\n",
              "      <td>0.945022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.275800</td>\n",
              "      <td>0.177723</td>\n",
              "      <td>0.946786</td>\n",
              "      <td>0.946561</td>\n",
              "      <td>0.946933</td>\n",
              "      <td>0.946786</td>\n",
              "      <td>0.926421</td>\n",
              "      <td>0.964783</td>\n",
              "      <td>0.948480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.232800</td>\n",
              "      <td>0.163707</td>\n",
              "      <td>0.953086</td>\n",
              "      <td>0.952963</td>\n",
              "      <td>0.953510</td>\n",
              "      <td>0.953086</td>\n",
              "      <td>0.936977</td>\n",
              "      <td>0.969233</td>\n",
              "      <td>0.952678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.235200</td>\n",
              "      <td>0.134407</td>\n",
              "      <td>0.959813</td>\n",
              "      <td>0.959677</td>\n",
              "      <td>0.960222</td>\n",
              "      <td>0.959813</td>\n",
              "      <td>0.947437</td>\n",
              "      <td>0.971472</td>\n",
              "      <td>0.960120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.145700</td>\n",
              "      <td>0.182042</td>\n",
              "      <td>0.953768</td>\n",
              "      <td>0.953383</td>\n",
              "      <td>0.955229</td>\n",
              "      <td>0.953768</td>\n",
              "      <td>0.937441</td>\n",
              "      <td>0.963954</td>\n",
              "      <td>0.958753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.256800</td>\n",
              "      <td>0.337103</td>\n",
              "      <td>0.919029</td>\n",
              "      <td>0.918983</td>\n",
              "      <td>0.927024</td>\n",
              "      <td>0.919029</td>\n",
              "      <td>0.891021</td>\n",
              "      <td>0.909940</td>\n",
              "      <td>0.955986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.189100</td>\n",
              "      <td>0.160607</td>\n",
              "      <td>0.961345</td>\n",
              "      <td>0.961097</td>\n",
              "      <td>0.962367</td>\n",
              "      <td>0.961345</td>\n",
              "      <td>0.948473</td>\n",
              "      <td>0.968762</td>\n",
              "      <td>0.966057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.228900</td>\n",
              "      <td>0.129324</td>\n",
              "      <td>0.963389</td>\n",
              "      <td>0.963368</td>\n",
              "      <td>0.963922</td>\n",
              "      <td>0.963389</td>\n",
              "      <td>0.954072</td>\n",
              "      <td>0.967074</td>\n",
              "      <td>0.968957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.144300</td>\n",
              "      <td>0.138896</td>\n",
              "      <td>0.968412</td>\n",
              "      <td>0.968386</td>\n",
              "      <td>0.968913</td>\n",
              "      <td>0.968412</td>\n",
              "      <td>0.960660</td>\n",
              "      <td>0.971315</td>\n",
              "      <td>0.973183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.142700</td>\n",
              "      <td>0.105549</td>\n",
              "      <td>0.974713</td>\n",
              "      <td>0.974670</td>\n",
              "      <td>0.974974</td>\n",
              "      <td>0.974713</td>\n",
              "      <td>0.970068</td>\n",
              "      <td>0.976750</td>\n",
              "      <td>0.977192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.155554</td>\n",
              "      <td>0.963814</td>\n",
              "      <td>0.963606</td>\n",
              "      <td>0.965151</td>\n",
              "      <td>0.963814</td>\n",
              "      <td>0.951684</td>\n",
              "      <td>0.964671</td>\n",
              "      <td>0.974463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.211600</td>\n",
              "      <td>0.181222</td>\n",
              "      <td>0.961601</td>\n",
              "      <td>0.961434</td>\n",
              "      <td>0.963183</td>\n",
              "      <td>0.961601</td>\n",
              "      <td>0.949162</td>\n",
              "      <td>0.960069</td>\n",
              "      <td>0.975070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.149400</td>\n",
              "      <td>0.120421</td>\n",
              "      <td>0.974968</td>\n",
              "      <td>0.974912</td>\n",
              "      <td>0.975631</td>\n",
              "      <td>0.974968</td>\n",
              "      <td>0.968791</td>\n",
              "      <td>0.974135</td>\n",
              "      <td>0.981809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.147300</td>\n",
              "      <td>0.089567</td>\n",
              "      <td>0.978544</td>\n",
              "      <td>0.978572</td>\n",
              "      <td>0.978953</td>\n",
              "      <td>0.978544</td>\n",
              "      <td>0.975142</td>\n",
              "      <td>0.973793</td>\n",
              "      <td>0.986780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.149800</td>\n",
              "      <td>0.102236</td>\n",
              "      <td>0.979140</td>\n",
              "      <td>0.979128</td>\n",
              "      <td>0.979763</td>\n",
              "      <td>0.979140</td>\n",
              "      <td>0.974909</td>\n",
              "      <td>0.976000</td>\n",
              "      <td>0.986476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.122000</td>\n",
              "      <td>0.196647</td>\n",
              "      <td>0.959557</td>\n",
              "      <td>0.959477</td>\n",
              "      <td>0.962462</td>\n",
              "      <td>0.959557</td>\n",
              "      <td>0.944668</td>\n",
              "      <td>0.949598</td>\n",
              "      <td>0.984163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.150100</td>\n",
              "      <td>0.144137</td>\n",
              "      <td>0.966965</td>\n",
              "      <td>0.966848</td>\n",
              "      <td>0.968892</td>\n",
              "      <td>0.966965</td>\n",
              "      <td>0.955315</td>\n",
              "      <td>0.961515</td>\n",
              "      <td>0.983715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.151400</td>\n",
              "      <td>0.078175</td>\n",
              "      <td>0.982205</td>\n",
              "      <td>0.982183</td>\n",
              "      <td>0.982672</td>\n",
              "      <td>0.982205</td>\n",
              "      <td>0.978352</td>\n",
              "      <td>0.980815</td>\n",
              "      <td>0.987384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.113200</td>\n",
              "      <td>0.052280</td>\n",
              "      <td>0.988421</td>\n",
              "      <td>0.988429</td>\n",
              "      <td>0.988543</td>\n",
              "      <td>0.988421</td>\n",
              "      <td>0.988432</td>\n",
              "      <td>0.986727</td>\n",
              "      <td>0.990127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.074600</td>\n",
              "      <td>0.073181</td>\n",
              "      <td>0.985781</td>\n",
              "      <td>0.985772</td>\n",
              "      <td>0.985919</td>\n",
              "      <td>0.985781</td>\n",
              "      <td>0.984361</td>\n",
              "      <td>0.986842</td>\n",
              "      <td>0.986113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.129100</td>\n",
              "      <td>0.069521</td>\n",
              "      <td>0.982801</td>\n",
              "      <td>0.982826</td>\n",
              "      <td>0.983216</td>\n",
              "      <td>0.982801</td>\n",
              "      <td>0.982226</td>\n",
              "      <td>0.980456</td>\n",
              "      <td>0.985795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.120600</td>\n",
              "      <td>0.116033</td>\n",
              "      <td>0.969945</td>\n",
              "      <td>0.969929</td>\n",
              "      <td>0.971797</td>\n",
              "      <td>0.969945</td>\n",
              "      <td>0.960711</td>\n",
              "      <td>0.962498</td>\n",
              "      <td>0.986578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='368' max='368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [368/368 00:38]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 2447/2447 [04:12<00:00,  9.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final results: {'eval_loss': 0.05242487043142319, 'eval_accuracy': 0.9873988931460196, 'eval_f1': 0.987404381923188, 'eval_precision': 0.9874708848884522, 'eval_recall': 0.9873988931460196, 'eval_f1_left': 0.9878064433320498, 'eval_f1_center': 0.9859333417817767, 'eval_f1_right': 0.9884733606557377, 'eval_runtime': 38.7448, 'eval_samples_per_second': 303.137, 'eval_steps_per_second': 9.498, 'epoch': 0.99241097489784}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = \"roberta-base\"  # Upgraded to a larger model\n",
        "LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n",
        "REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/coding/results\"\n",
        "LOGGING_DIR = \"/content/drive/MyDrive/coding/logs\"\n",
        "RESULTS_PATH = \"/content/drive/MyDrive/coding/results/predictions.csv\"\n",
        "CACHE_DIR = \"/content/drive/MyDrive/coding/results/cache\"\n",
        "DATASET_CACHE_DIR = \"/content/drive/MyDrive/coding/result/dataset_cache\"\n",
        "# Ensure directories exist\n",
        "for directory in [OUTPUT_DIR, LOGGING_DIR, CACHE_DIR, DATASET_CACHE_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Set device and optimize memory usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Optimize memory usage based on available hardware\n",
        "def optimize_memory():\n",
        "    config = {\n",
        "        \"batch_size\": 8,\n",
        "        \"eval_batch_size\": 16,\n",
        "        \"num_workers\": min(os.cpu_count() - 1, 4) if os.cpu_count() > 1 else 0,\n",
        "        \"gradient_accumulation_steps\": 2,\n",
        "        \"mixed_precision\": False\n",
        "    }\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        logger.info(f\"GPU Memory: {total_memory:.2f} GB\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if total_memory > 10:\n",
        "            config[\"batch_size\"] = 16\n",
        "            config[\"eval_batch_size\"] = 32\n",
        "            config[\"gradient_accumulation_steps\"] = 1\n",
        "        elif total_memory < 4:\n",
        "            config[\"batch_size\"] = 4\n",
        "            config[\"eval_batch_size\"] = 8\n",
        "            config[\"gradient_accumulation_steps\"] = 4\n",
        "\n",
        "        config[\"mixed_precision\"] = True\n",
        "\n",
        "    return config\n",
        "\n",
        "memory_config = optimize_memory()\n",
        "\n",
        "# ---------------- Text Processing Functions ----------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "@lru_cache(maxsize=1024)\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text) or text is None:\n",
        "        return \"\"\n",
        "    if isinstance(text, list):\n",
        "        text = ' '.join(str(t).lower().strip() for t in text if isinstance(t, str))\n",
        "    text = str(text).lower().strip()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def combine_text(df, text_cols, weights=None):\n",
        "    if weights is None:\n",
        "        weights = {\"text\": 1.0, \"topic\": 0.5, \"article\": 0.8, \"biased_words\": 2.0}\n",
        "    processed_cols = {}\n",
        "    for col in text_cols:\n",
        "        if col in df.columns:\n",
        "            processed_cols[col] = df[col].apply(preprocess_text)\n",
        "        else:\n",
        "            processed_cols[col] = pd.Series([\"\"] * len(df))\n",
        "\n",
        "    combined_series = processed_cols[text_cols[0]].copy()\n",
        "    for col in text_cols[1:]:\n",
        "        mask = processed_cols[col] != \"\"\n",
        "        weighted_text = processed_cols[col][mask].apply(lambda x: (x + \" \") * int(weights[col])).str.strip()\n",
        "        combined_series[mask] = combined_series[mask] + \" \" + weighted_text\n",
        "\n",
        "    return combined_series\n",
        "\n",
        "def create_data_from_batch(batch_df, tokenizer, max_length=256):\n",
        "    texts = batch_df[\"combined_input\"].tolist()\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = None\n",
        "    if \"label\" in batch_df:\n",
        "        labels = torch.tensor(batch_df[\"label\"].tolist())\n",
        "    return {\n",
        "        \"input_ids\": encodings[\"input_ids\"],\n",
        "        \"attention_mask\": encodings[\"attention_mask\"],\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# ---------------- Load and Process Data ----------------\n",
        "def load_data(file_path):\n",
        "    cache_file = os.path.join(DATASET_CACHE_DIR, f\"{os.path.basename(file_path)}.processed.pkl\")\n",
        "    if os.path.exists(cache_file):\n",
        "        logger.info(f\"Loading processed data from cache: {cache_file}\")\n",
        "        return pd.read_pickle(cache_file)\n",
        "\n",
        "    logger.info(\"Loading and processing data\")\n",
        "    try:\n",
        "        if file_path.endswith('.xlsx'):\n",
        "            df = pd.read_excel(file_path, engine='openpyxl')\n",
        "        elif file_path.endswith('.csv'):\n",
        "            df = pd.read_csv(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n",
        "    logger.info(\"Class distribution:\")\n",
        "    logger.info(df[\"type\"].value_counts())\n",
        "\n",
        "    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n",
        "    df[\"combined_input\"] = combine_text(df, text_cols)\n",
        "    df[\"label\"] = df[\"type\"].map(LABEL_MAP)\n",
        "\n",
        "    df.to_pickle(cache_file)\n",
        "    logger.info(f\"Processed data cached to: {cache_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---------------- Dataset Classes ----------------\n",
        "class PoliticalBiasDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=256):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.has_labels = \"label\" in df.columns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"combined_input\": self.df.iloc[idx][\"combined_input\"]}\n",
        "        if self.has_labels:\n",
        "            item[\"label\"] = self.df.iloc[idx][\"label\"]\n",
        "        return item\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch_df = pd.DataFrame(batch)\n",
        "        return create_data_from_batch(batch_df, self.tokenizer, self.max_length)\n",
        "\n",
        "# ---------------- Metrics ----------------\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    class_report = classification_report(labels, preds, target_names=LABEL_MAP.keys(), output_dict=True)\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }\n",
        "    for cls in LABEL_MAP.keys():\n",
        "        metrics[f\"f1_{cls}\"] = class_report[cls]['f1-score']\n",
        "    return metrics\n",
        "\n",
        "# ---------------- Main Pipeline ----------------\n",
        "def main(file_path):\n",
        "    logger.info(\"Starting main pipeline\")\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    logger.info(\"Splitting data\")\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "    logger.info(f\"Train set: {len(train_df)} samples\")\n",
        "    logger.info(f\"Validation set: {len(val_df)} samples\")\n",
        "    logger.info(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    logger.info(f\"Loading tokenizer and model: {MODEL_NAME}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=len(LABEL_MAP),\n",
        "            cache_dir=CACHE_DIR,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model {MODEL_NAME}: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    train_dataset = PoliticalBiasDataset(train_df, tokenizer, max_length=256)\n",
        "    val_dataset = PoliticalBiasDataset(val_df, tokenizer, max_length=256)\n",
        "    test_dataset = PoliticalBiasDataset(test_df, tokenizer, max_length=256)\n",
        "\n",
        "    # Compute class weights for imbalanced data\n",
        "    class_weights = compute_class_weight(\"balanced\", classes=np.array([0, 1, 2]), y=train_df[\"label\"])\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "    logger.info(\"Setting up training arguments\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        logging_dir=LOGGING_DIR,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        per_device_train_batch_size=memory_config[\"batch_size\"],\n",
        "        per_device_eval_batch_size=memory_config[\"eval_batch_size\"],\n",
        "        gradient_accumulation_steps=memory_config[\"gradient_accumulation_steps\"],\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=3e-5,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"none\",\n",
        "        fp16=memory_config[\"mixed_precision\"],\n",
        "        dataloader_num_workers=memory_config[\"num_workers\"],\n",
        "        remove_unused_columns=False,\n",
        "        disable_tqdm=False\n",
        "    )\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        return train_dataset.collate_fn(batch)\n",
        "\n",
        "    # Custom Trainer with class weights, updated to handle num_items_in_batch\n",
        "    class WeightedTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "            labels = inputs.get(\"labels\")\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.get(\"logits\")\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    logger.info(\"Initializing Trainer\")\n",
        "    trainer = WeightedTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=collate_fn,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting training\")\n",
        "    trainer.train()\n",
        "\n",
        "    logger.info(\"Saving model and tokenizer\")\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
        "\n",
        "    logger.info(\"Evaluating on test set\")\n",
        "    test_results = trainer.evaluate(test_dataset)\n",
        "    logger.info(f\"Test results: {test_results}\")\n",
        "\n",
        "    logger.info(\"Making predictions on all data\")\n",
        "    full_dataset = PoliticalBiasDataset(df[[\"combined_input\"]], tokenizer, max_length=256)\n",
        "    prediction_dataloader = torch.utils.data.DataLoader(\n",
        "        full_dataset,\n",
        "        batch_size=memory_config[\"eval_batch_size\"],\n",
        "        collate_fn=full_dataset.collate_fn,\n",
        "        num_workers=memory_config[\"num_workers\"],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    all_predictions = []\n",
        "    all_confidences = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(prediction_dataloader, desc=\"Predicting\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            permission_mask = batch[\"attention_mask\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=permission_mask)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            batch_confidences, batch_preds = torch.max(probabilities, dim=1)\n",
        "            all_predictions.extend(batch_preds.cpu().numpy())\n",
        "            all_confidences.extend(batch_confidences.cpu().numpy())\n",
        "\n",
        "    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in all_predictions]\n",
        "    df[\"confidence_score\"] = all_confidences\n",
        "    df[\"is_confident\"] = df[\"confidence_score\"] >= 0.95\n",
        "    df.to_csv(RESULTS_PATH, index=False)\n",
        "    logger.info(f\"Predictions with confidence scores saved to {RESULTS_PATH}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return test_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = \"/content/drive/MyDrive/coding/complete_balanced_data.csv\"\n",
        "    results = main(input_file_path)\n",
        "    print(f\"Final results: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BylnUnSBMmBY"
      },
      "source": [
        "### Test roberta model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHmwqveRzSrz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAOF22bEvbz1",
        "outputId": "5f8427e4-815c-493e-c7e1-0176740c0bfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text: ['As wildfires rage across California, floods displace thousands in the Midwest, and heatwaves scorch cities from Texas to New York, the evidence is undeniable: the climate crisis is no longer a distant threat—it’s here. And yet, as communities suffer and ecosystems collapse, fossil fuel corporations continue to post record-breaking profits, protected by conservative politicians and a global system rigged in their favor.\\nIn 2024 alone, the five largest oil companies reported over $200 billion in profits. Instead of investing in renewable energy or helping vulnerable communities transition to a green economy, these corporations funneled billions into stock buybacks and executive bonuses. Their message is clear: profits come before people, and the planet can burn so long as the shareholders stay rich.\\nEven more alarming is the political shielding they receive from right-wing lawmakers, many of whom deny climate science altogether. Republican leaders in Congress have repeatedly blocked climate legislation, gutted the Environmental Protection Agency’s regulatory powers, and prioritized drilling permits over clean air and water.\\nMeanwhile, climate activists—many of them youth, Indigenous leaders, and marginalized communities—continue to face police repression, surveillance, and criminalization. Peaceful protesters at pipeline sites are arrested, while oil spills and environmental destruction go unpunished.\\nWe need a Green New Deal-level transformation: bold investments in wind, solar, and green infrastructure; the creation of millions of unionized green jobs; and climate reparations for communities hit hardest by pollution and environmental racism.\\nThe time for delay is over. The time to act is now.']\n",
            "Predicted Bias: left \n",
            " confidence score:(0.9996)\n",
            "\n",
            "Text: [\"The United States thrives when government steps back and lets free enterprise lead. In recent years, however, progressive lawmakers have increasingly pushed for regulation, redistribution, and intervention that stifles innovation and discourages hard work.\\nFrom overreaching environmental mandates to government-controlled healthcare proposals, the left continues to champion policies that prioritize bureaucracy over results. These moves are not only anti-business—they’re anti-American.\\nAmerica's economic engine runs best when the private sector is free to create, compete, and grow. Small business owners across the country are already struggling with inflation and labor shortages—problems worsened by excessive government interference and rising taxes.\\nWe must return to policies that reward productivity, protect property rights, and uphold free-market values. Deregulation, tax reform, and energy independence will not only restore our economy—they’ll renew our national spirit.\\n\"]\n",
            "Predicted Bias: right \n",
            " confidence score:(0.9582)\n",
            "\n",
            "Text: ['As artificial intelligence tools become increasingly integrated into everyday life—from health diagnostics to criminal justice systems—Democratic and Republican lawmakers alike are recognizing the need for clear regulatory frameworks.\\nA bipartisan group in Congress recently introduced the American AI Responsibility Act, aiming to address transparency, data privacy, and algorithmic bias. While the bill doesn’t go as far as some activists demand, it marks an important step toward balancing innovation with accountability.\\nTech CEOs have expressed cautious support, stating that some regulation is needed to maintain public trust, but they warn against overregulation that could drive development offshore.\\nExperts agree: regulation must be careful, measured, and informed by the science—not by political theater. While divisions remain, the shared concern over AI’s risks may offer a rare opportunity for consensus in Washington.\\n']\n",
            "Predicted Bias: left \n",
            " confidence score:(0.9992)\n",
            "\n",
            "Text: ['In yet another blow to working-class Americans, Senate Republicans have blocked legislation that would raise the federal minimum wage to $17 per hour by 2027. With wages stagnant and inflation hitting food, rent, and transportation costs, the move is being widely condemned by labor leaders and economists.\\nThe current $7.25 minimum wage has not been raised since 2009, despite historic gains in productivity and corporate profits. Over 60% of Americans support a raise, but Republican lawmakers claim it would “hurt small businesses”—an argument that many economists say is overblown.\\nIn reality, the refusal to raise wages preserves exploitative systems where billion-dollar corporations rely on underpaid workers while CEO salaries skyrocket.\\nThis is not just about economics—it’s about dignity. Every American who works full-time should be able to afford basic necessities. Congress’s failure to act is a moral failure, and it’s up to voters to hold them accountable.']\n",
            "Predicted Bias: left \n",
            " confidence score:(0.9993)\n",
            "\n",
            "Text: ['The southern border has long been a flashpoint in American politics, but recent data shows that tougher enforcement and advanced surveillance technology are yielding results. Illegal crossings dropped 30% in the first quarter of 2025 compared to the previous year, according to Homeland Security reports.\\nUnder the new measures, authorities have deployed AI-powered drones, reinforced border fencing, and accelerated asylum screening procedures. Critics on the left say the policies are “inhumane,” but officials argue they are necessary to protect national sovereignty and public safety.\\nDrug seizures have also increased, particularly fentanyl shipments originating from cartels that exploit weak border points. Law enforcement agencies say the new tools and funding are making a significant impact.\\nThe Biden administration was slow to act early in its term, but this policy shift marks a necessary correction. The right to immigrate must be balanced with the rule of law—and American citizens deserve to feel safe and secure in their own country.\\n']\n",
            "Predicted Bias: left \n",
            " confidence score:(0.9990)\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/coding/results\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "test_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
        "\n",
        "samples = [\n",
        "[\"\"\"As wildfires rage across California, floods displace thousands in the Midwest, and heatwaves scorch cities from Texas to New York, the evidence is undeniable: the climate crisis is no longer a distant threat—it’s here. And yet, as communities suffer and ecosystems collapse, fossil fuel corporations continue to post record-breaking profits, protected by conservative politicians and a global system rigged in their favor.\n",
        "In 2024 alone, the five largest oil companies reported over $200 billion in profits. Instead of investing in renewable energy or helping vulnerable communities transition to a green economy, these corporations funneled billions into stock buybacks and executive bonuses. Their message is clear: profits come before people, and the planet can burn so long as the shareholders stay rich.\n",
        "Even more alarming is the political shielding they receive from right-wing lawmakers, many of whom deny climate science altogether. Republican leaders in Congress have repeatedly blocked climate legislation, gutted the Environmental Protection Agency’s regulatory powers, and prioritized drilling permits over clean air and water.\n",
        "Meanwhile, climate activists—many of them youth, Indigenous leaders, and marginalized communities—continue to face police repression, surveillance, and criminalization. Peaceful protesters at pipeline sites are arrested, while oil spills and environmental destruction go unpunished.\n",
        "We need a Green New Deal-level transformation: bold investments in wind, solar, and green infrastructure; the creation of millions of unionized green jobs; and climate reparations for communities hit hardest by pollution and environmental racism.\n",
        "The time for delay is over. The time to act is now.\"\"\"\n",
        "],\n",
        "[\"\"\"The United States thrives when government steps back and lets free enterprise lead. In recent years, however, progressive lawmakers have increasingly pushed for regulation, redistribution, and intervention that stifles innovation and discourages hard work.\n",
        "From overreaching environmental mandates to government-controlled healthcare proposals, the left continues to champion policies that prioritize bureaucracy over results. These moves are not only anti-business—they’re anti-American.\n",
        "America's economic engine runs best when the private sector is free to create, compete, and grow. Small business owners across the country are already struggling with inflation and labor shortages—problems worsened by excessive government interference and rising taxes.\n",
        "We must return to policies that reward productivity, protect property rights, and uphold free-market values. Deregulation, tax reform, and energy independence will not only restore our economy—they’ll renew our national spirit.\n",
        "\"\"\"],\n",
        "[\"\"\"As artificial intelligence tools become increasingly integrated into everyday life—from health diagnostics to criminal justice systems—Democratic and Republican lawmakers alike are recognizing the need for clear regulatory frameworks.\n",
        "A bipartisan group in Congress recently introduced the American AI Responsibility Act, aiming to address transparency, data privacy, and algorithmic bias. While the bill doesn’t go as far as some activists demand, it marks an important step toward balancing innovation with accountability.\n",
        "Tech CEOs have expressed cautious support, stating that some regulation is needed to maintain public trust, but they warn against overregulation that could drive development offshore.\n",
        "Experts agree: regulation must be careful, measured, and informed by the science—not by political theater. While divisions remain, the shared concern over AI’s risks may offer a rare opportunity for consensus in Washington.\n",
        "\"\"\"],\n",
        "[\"\"\"In yet another blow to working-class Americans, Senate Republicans have blocked legislation that would raise the federal minimum wage to $17 per hour by 2027. With wages stagnant and inflation hitting food, rent, and transportation costs, the move is being widely condemned by labor leaders and economists.\n",
        "The current $7.25 minimum wage has not been raised since 2009, despite historic gains in productivity and corporate profits. Over 60% of Americans support a raise, but Republican lawmakers claim it would “hurt small businesses”—an argument that many economists say is overblown.\n",
        "In reality, the refusal to raise wages preserves exploitative systems where billion-dollar corporations rely on underpaid workers while CEO salaries skyrocket.\n",
        "This is not just about economics—it’s about dignity. Every American who works full-time should be able to afford basic necessities. Congress’s failure to act is a moral failure, and it’s up to voters to hold them accountable.\"\"\"],\n",
        " [\"\"\"The southern border has long been a flashpoint in American politics, but recent data shows that tougher enforcement and advanced surveillance technology are yielding results. Illegal crossings dropped 30% in the first quarter of 2025 compared to the previous year, according to Homeland Security reports.\n",
        "Under the new measures, authorities have deployed AI-powered drones, reinforced border fencing, and accelerated asylum screening procedures. Critics on the left say the policies are “inhumane,” but officials argue they are necessary to protect national sovereignty and public safety.\n",
        "Drug seizures have also increased, particularly fentanyl shipments originating from cartels that exploit weak border points. Law enforcement agencies say the new tools and funding are making a significant impact.\n",
        "The Biden administration was slow to act early in its term, but this policy shift marks a necessary correction. The right to immigrate must be balanced with the rule of law—and American citizens deserve to feel safe and secure in their own country.\n",
        "\"\"\"]\n",
        "]\n",
        "\n",
        "for text in samples:\n",
        "    output = test_pipeline(text, truncation=True, max_length=512)\n",
        "    sorted_output = sorted(output[0], key=lambda x: x[\"score\"], reverse=True)\n",
        "    top_label = sorted_output[0]\n",
        "\n",
        "    label_map = {\n",
        "    0: \"left\", 1: \"center\", 2: \"right\"}\n",
        "    # Extract the numeric part of the label like 'LABEL_1' -> 1\n",
        "    label_index = int(top_label['label'].split('_')[-1])\n",
        "    readable_label = label_map[label_index]\n",
        "\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Predicted Bias: {readable_label} \\n confidence score:({top_label['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6w1qD0iCE4A"
      },
      "source": [
        "## Improved version with openNLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-8Pr2iViCQG5"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from functools import lru_cache\n",
        "import spacy\n",
        "from torch import nn\n",
        "import optuna\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5yxqwYjIaal",
        "outputId": "8e4baf6e-f5af-4007-c7ef-b0bf2f80d78c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = \"launch/politics\"\n",
        "LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n",
        "REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/coding/bias_results\"\n",
        "LOGGING_DIR = \"/content/drive/MyDrive/coding/bia_logs\"\n",
        "RESULTS_PATH = \"/content/drive/MyDrive/coding/bias_results/predictions.csv\"\n",
        "CACHE_DIR = \"/content/drive/MyDrive/coding/bias_results/cache\"\n",
        "DATASET_CACHE_DIR = \"/content/drive/MyDrive/coding/bias_result/dataset_cache\"\n",
        "MAX_LENGTH = 128\n",
        "PRELEMMATIZE = True\n",
        "RUN_HYPERPARAMETER_TUNING = True\n",
        "N_OPTUNA_TRIALS = 10\n",
        "\n",
        "# Ensure directories exist\n",
        "for directory in [OUTPUT_DIR, LOGGING_DIR, CACHE_DIR, DATASET_CACHE_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize spaCy\n",
        "try:\n",
        "    nlp_spacy = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "    logger.info(\"spaCy initialized with en_core_web_sm\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to load spaCy model: {e}. Install with: python -m spacy download en_core_web_sm\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Optimize memory usage\n",
        "def optimize_memory():\n",
        "    config = {\n",
        "        \"batch_size\": 8,\n",
        "        \"eval_batch_size\": 16,\n",
        "        \"num_workers\": min(os.cpu_count() - 1, 4) if os.cpu_count() > 1 else 0,\n",
        "        \"gradient_accumulation_steps\": 2,\n",
        "        \"mixed_precision\": False\n",
        "    }\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        logger.info(f\"GPU Memory: {total_memory:.2f} GB\")\n",
        "        torch.cuda.empty_cache()\n",
        "        if total_memory > 10:\n",
        "            config[\"batch_size\"] = 16\n",
        "            config[\"eval_batch_size\"] = 32\n",
        "            config[\"gradient_accumulation_steps\"] = 1\n",
        "        elif total_memory < 4:\n",
        "            config[\"batch_size\"] = 4\n",
        "            config[\"eval_batch_size\"] = 8\n",
        "            config[\"gradient_accumulation_steps\"] = 4\n",
        "        config[\"mixed_precision\"] = True\n",
        "    return config\n",
        "\n",
        "memory_config = optimize_memory()\n",
        "\n",
        "# Text Processing Functions\n",
        "@lru_cache(maxsize=1024)\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text) or text is None:\n",
        "        return \"\"\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    try:\n",
        "        doc = nlp_spacy(text)\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "        return ' '.join(lemmas)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"spaCy error: {e}. Returning cleaned text.\")\n",
        "        return text\n",
        "\n",
        "def combine_text(df, text_cols):\n",
        "    available_cols = [col for col in text_cols if col in df.columns]\n",
        "    if not available_cols:\n",
        "        logger.error(\"No valid text columns found\")\n",
        "        sys.exit(1)\n",
        "    processed_cols = {col: df[col].apply(preprocess_text) if col in df.columns else pd.Series([\"\"] * len(df)) for col in text_cols}\n",
        "    combined_series = processed_cols[text_cols[0]].copy()\n",
        "    for col in text_cols[1:]:\n",
        "        mask = processed_cols[col] != \"\"\n",
        "        combined_series[mask] = combined_series[mask] + \" \" + processed_cols[col][mask]\n",
        "    return combined_series\n",
        "\n",
        "def create_data_from_batch(batch_df, tokenizer, max_length=MAX_LENGTH):\n",
        "    texts = batch_df[\"combined_input\"].tolist()\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = None\n",
        "    if \"label\" in batch_df:\n",
        "        labels = torch.tensor(batch_df[\"label\"].tolist())\n",
        "    return {\n",
        "        \"input_ids\": encodings[\"input_ids\"],\n",
        "        \"attention_mask\": encodings[\"attention_mask\"],\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "# Load and Process Data\n",
        "def load_data(file_path):\n",
        "    cache_file = os.path.join(DATASET_CACHE_DIR, f\"{os.path.basename(file_path)}.processed.pkl\")\n",
        "    if os.path.exists(cache_file):\n",
        "        logger.info(f\"Loading processed data from cache: {cache_file}\")\n",
        "        return pd.read_pickle(cache_file)\n",
        "\n",
        "    logger.info(\"Loading and processing data\")\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"Input file not found: {file_path}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        if file_path.endswith('.xlsx'):\n",
        "            df = pd.read_excel(file_path, engine='openpyxl')\n",
        "        elif file_path.endswith('.csv'):\n",
        "            df = pd.read_csv(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    required_cols = [\"text\", \"type\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        logger.error(f\"Missing required columns: {required_cols}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n",
        "    logger.info(\"Class distribution:\")\n",
        "    logger.info(df[\"type\"].value_counts())\n",
        "\n",
        "    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n",
        "    if PRELEMMATIZE:\n",
        "        logger.info(\"Pre-lemmatizing dataset\")\n",
        "        for col in text_cols:\n",
        "            if col in df.columns:\n",
        "                df[f\"{col}_preprocessed\"] = df[col].apply(preprocess_text)\n",
        "        df[\"combined_input\"] = combine_text(df, [f\"{col}_preprocessed\" for col in text_cols])\n",
        "    else:\n",
        "        df[\"combined_input\"] = combine_text(df, text_cols)\n",
        "\n",
        "    df[\"label\"] = df[\"type\"].map(LABEL_MAP)\n",
        "\n",
        "    logger.info(f\"Sample data:\\n{df[['combined_input', 'type']].head(5).to_string()}\")\n",
        "    df.to_pickle(cache_file)\n",
        "    logger.info(f\"Processed data cached to: {cache_file}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Dataset Class\n",
        "class PoliticalBiasDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=MAX_LENGTH):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.has_labels = \"label\" in df.columns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"combined_input\": self.df.iloc[idx][\"combined_input\"]}\n",
        "        if self.has_labels:\n",
        "            item[\"label\"] = self.df.iloc[idx][\"label\"]\n",
        "        return item\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch_df = pd.DataFrame(batch)\n",
        "        return create_data_from_batch(batch_df, self.tokenizer, self.max_length)\n",
        "\n",
        "# Custom Model with Additional Dense Layer\n",
        "class CustomRobertaForSequenceClassification(AutoModelForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0][:, 0, :]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output = self.dense(sequence_output)\n",
        "        sequence_output = self.relu(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    class_report = classification_report(labels, preds, target_names=LABEL_MAP.keys(), output_dict=True)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=LABEL_MAP.keys(), yticklabels=LABEL_MAP.keys())\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    cm_path = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')\n",
        "    plt.savefig(cm_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"Confusion matrix saved to {cm_path}\")\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }\n",
        "    for cls in LABEL_MAP.keys():\n",
        "        metrics[f\"f1_{cls}\"] = class_report[cls]['f1-score']\n",
        "\n",
        "    logger.info(f\"Classification Report:\\n{classification_report(labels, preds, target_names=LABEL_MAP.keys())}\")\n",
        "    return metrics\n",
        "\n",
        "# Custom Trainer with Class Weights\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[\"logits\"]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Hyperparameter Tuning with Optuna\n",
        "def run_hyperparameter_tuning(model, tokenizer, train_dataset, val_dataset, class_weights, output_dir, logging_dir, n_trials=N_OPTUNA_TRIALS):\n",
        "    def objective(trial):\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"{output_dir}/trial_{trial.number}\",\n",
        "            logging_dir=logging_dir,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=100,\n",
        "            per_device_train_batch_size=trial.suggest_categorical(\"batch_size\", [4, 8, 16]),\n",
        "            per_device_eval_batch_size=16,\n",
        "            gradient_accumulation_steps=trial.suggest_int(\"gradient_accumulation_steps\", 1, 4),\n",
        "            num_train_epochs=trial.suggest_int(\"num_epochs\", 5, 15),\n",
        "            learning_rate=trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "            warmup_steps=trial.suggest_int(\"warmup_steps\", 50, 200),\n",
        "            weight_decay=trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
        "            logging_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=1,\n",
        "            report_to=\"none\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            dataloader_num_workers=min(os.cpu_count() - 1, 4) if os.cpu_count() > 1 else 0,\n",
        "            remove_unused_columns=False,\n",
        "            disable_tqdm=False,\n",
        "            gradient_checkpointing=True,\n",
        "            lr_scheduler_type=\"linear\"\n",
        "        )\n",
        "\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            data_collator=train_dataset.collate_fn,\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "            class_weights=class_weights\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        eval_results = trainer.evaluate()\n",
        "\n",
        "        if trial.number == 0 or eval_results[\"eval_f1\"] > trial.study.best_value:\n",
        "            trainer.save_model(f\"{output_dir}/best_model\")\n",
        "            tokenizer.save_pretrained(f\"{output_dir}/best_model\")\n",
        "            logger.info(f\"Saved best model from trial {trial.number} with F1: {eval_results['eval_f1']:.4f}\")\n",
        "\n",
        "        return eval_results[\"eval_f1\"]\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    logger.info(f\"Best hyperparameters: {study.best_params}\")\n",
        "    logger.info(f\"Best F1-score: {study.best_value:.4f}\")\n",
        "    return study.best_params\n",
        "\n",
        "# Main Pipeline\n",
        "def main(file_path):\n",
        "    logger.info(\"Starting main pipeline\")\n",
        "\n",
        "    # Load and prepare data\n",
        "    df = load_data(file_path)\n",
        "\n",
        "    # Train-Test Split\n",
        "    logger.info(\"Splitting data\")\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "    logger.info(f\"Train set: {len(train_df)} samples\")\n",
        "    logger.info(f\"Validation set: {len(val_df)} samples\")\n",
        "    logger.info(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "    # Load Tokenizer and Model\n",
        "    logger.info(f\"Loading tokenizer and model: {MODEL_NAME}\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
        "        model = CustomRobertaForSequenceClassification.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            num_labels=len(LABEL_MAP),\n",
        "            cache_dir=CACHE_DIR,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        logger.info(f\"Successfully loaded model: {MODEL_NAME}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {MODEL_NAME}: {e}\")\n",
        "        raise\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_df[\"label\"]), y=train_df[\"label\"])\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    logger.info(f\"Class weights: {class_weights.tolist()}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = PoliticalBiasDataset(train_df, tokenizer)\n",
        "    val_dataset = PoliticalBiasDataset(val_df, tokenizer)\n",
        "    test_dataset = PoliticalBiasDataset(test_df, tokenizer)\n",
        "\n",
        "    # Run hyperparameter tuning or default training\n",
        "    if RUN_HYPERPARAMETER_TUNING:\n",
        "        logger.info(\"Running hyperparameter tuning with Optuna\")\n",
        "        best_params = run_hyperparameter_tuning(\n",
        "            model, tokenizer, train_dataset, val_dataset, class_weights, OUTPUT_DIR, LOGGING_DIR\n",
        "        )\n",
        "\n",
        "        # Update training arguments with best hyperparameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            logging_dir=LOGGING_DIR,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=100,\n",
        "            per_device_train_batch_size=best_params[\"batch_size\"],\n",
        "            per_device_eval_batch_size=16,\n",
        "            gradient_accumulation_steps=best_params[\"gradient_accumulation_steps\"],\n",
        "            num_train_epochs=best_params[\"num_epochs\"],\n",
        "            learning_rate=best_params[\"learning_rate\"],\n",
        "            warmup_steps=best_params[\"warmup_steps\"],\n",
        "            weight_decay=best_params[\"weight_decay\"],\n",
        "            logging_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=1,\n",
        "            report_to=\"none\",\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            dataloader_num_workers=min(os.cpu_count() - 1, 4) if os.cpu_count() > 1 else 0,\n",
        "            remove_unused_columns=False,\n",
        "            disable_tqdm=False,\n",
        "            gradient_checkpointing=True,\n",
        "            lr_scheduler_type=\"linear\"\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Using default training arguments\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            logging_dir=LOGGING_DIR,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=100,\n",
        "            per_device_train_batch_size=memory_config[\"batch_size\"],\n",
        "            per_device_eval_batch_size=memory_config[\"eval_batch_size\"],\n",
        "            gradient_accumulation_steps=memory_config[\"gradient_accumulation_steps\"],\n",
        "            num_train_epochs=10,\n",
        "            learning_rate=2e-5,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            logging_steps=50,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            save_total_limit=1,\n",
        "            report_to=\"none\",\n",
        "            fp16=memory_config[\"mixed_precision\"],\n",
        "            dataloader_num_workers=memory_config[\"num_workers\"],\n",
        "            remove_unused_columns=False,\n",
        "            disable_tqdm=False,\n",
        "            gradient_checkpointing=True,\n",
        "            lr_scheduler_type=\"linear\"\n",
        "        )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    logger.info(\"Initializing Trainer\")\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=train_dataset.collate_fn,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    logger.info(\"Starting training\")\n",
        "    trainer.train()\n",
        "    trainer.save_metrics(\"all\", trainer.metrics)\n",
        "    logger.info(f\"Training metrics saved to {LOGGING_DIR}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    logger.info(\"Saving model and tokenizer\")\n",
        "    model.save_pretrained(OUTPUT_DIR)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    logger.info(\"Evaluating on test set\")\n",
        "    test_results = trainer.evaluate(test_dataset)\n",
        "    logger.info(f\"Test results: {test_results}\")\n",
        "    for metric, value in test_results.items():\n",
        "        logger.info(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    logger.info(\"Making predictions on all data\")\n",
        "    full_dataset = PoliticalBiasDataset(df[[\"combined_input\"]], tokenizer)\n",
        "    prediction_dataloader = DataLoader(\n",
        "        full_dataset,\n",
        "        batch_size=memory_config[\"eval_batch_size\"],\n",
        "        collate_fn=full_dataset.collate_fn,\n",
        "        num_workers=memory_config[\"num_workers\"],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    all_predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(prediction_dataloader, desc=\"Predicting\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs[\"logits\"]\n",
        "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_predictions.extend(batch_preds)\n",
        "\n",
        "    # Save predictions\n",
        "    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in all_predictions]\n",
        "    df.to_csv(RESULTS_PATH, index=False)\n",
        "    logger.info(f\"Predictions saved to {RESULTS_PATH}\")\n",
        "\n",
        "    # Save misclassified examples\n",
        "    misclassified = df[df[\"label\"] != df[\"predicted_bias_category\"].map(LABEL_MAP)]\n",
        "    misclassified_path = os.path.join(OUTPUT_DIR, \"misclassified_examples.csv\")\n",
        "    misclassified.to_csv(misclassified_path, index=False)\n",
        "    logger.info(f\"Misclassified examples saved to {misclassified_path}\")\n",
        "\n",
        "    # Clean up GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return test_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = \"/content/drive/MyDrive/coding/complete_balanced_data.csv\"\n",
        "    results = main(input_file_path)\n",
        "    print(f\"Final results: {results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "Nil_2PrVIfhL",
        "outputId": "be80c769-c5f2-4837-9b92-2b47217bcd31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1be50b7405e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0minput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/coding/complete_balanced_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final results: {results}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1be50b7405e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# Load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m# Train-Test Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1be50b7405e4>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{col}_preprocessed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"combined_input\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{col}_preprocessed\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1be50b7405e4>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                     \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/lemmatizer.py\u001b[0m in \u001b[0;36mrule_lemmatize\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0muniv_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muniv_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"space\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muniv_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.utils import resample\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n",
    "    TrainingArguments, pipeline\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Step 1: Read Data ----------------\n",
    "#file_path = \"/content/drive/MyDrive/internship_vaali_infotech/final_labels_MBIC.xlsx\"\n",
    "file_path=\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# ---------------- Step 2: Bias----------------\n",
    "def rebalance_three_categories(df):\n",
    "    # Assume 'type' contains only 'left', 'center', 'right'\n",
    "    df_left = df[df[\"type\"] == \"left\"]\n",
    "    df_center = df[df[\"type\"] == \"center\"]\n",
    "    df_right = df[df[\"type\"] == \"right\"]\n",
    "\n",
    "    # Find the smallest group size\n",
    "    min_count = min(len(df_left), len(df_center), len(df_right))\n",
    "\n",
    "    # Downsample all to the same size\n",
    "    df_left = resample(df_left, n_samples=min_count, replace=False, random_state=42)\n",
    "    df_center = resample(df_center, n_samples=min_count, replace=False, random_state=42)\n",
    "    df_right = resample(df_right, n_samples=min_count, replace=False, random_state=42)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    df_balanced = pd.concat([df_left, df_center, df_right])\n",
    "    return df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Apply new balancing\n",
    "df = rebalance_three_categories(df)\n",
    "\n",
    "def bias_type_column(df, bias_factor=0.4, target_type='center'):\n",
    "    if 'type' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'type' column\")\n",
    "    \n",
    "    df_target = df[df['type'] == target_type]\n",
    "    df_other = df[df['type'] != target_type]\n",
    "    \n",
    "    n_target = len(df_target)\n",
    "    \n",
    "    # Ensure desired_other is defined properly\n",
    "    if n_target > 0:\n",
    "        desired_other = int((n_target * (1 - bias_factor)) / bias_factor)\n",
    "    else:\n",
    "        raise ValueError(\"Target type has no entries in the DataFrame.\")\n",
    "\n",
    "    # Ensure desired_other is at least 1\n",
    "    desired_other = max(1, desired_other)\n",
    "\n",
    "    if len(df_other) > desired_other:\n",
    "        df_other = resample(df_other, n_samples=desired_other, replace=False, random_state=42)\n",
    "    else:\n",
    "        additional_target = int((len(df_other) * bias_factor) / (1 - bias_factor)) - n_target\n",
    "        if additional_target > 0:\n",
    "            df_target = pd.concat([\n",
    "                df_target,\n",
    "                resample(df_target, n_samples=additional_target, replace=True, random_state=42)\n",
    "            ])\n",
    "    \n",
    "    return pd.concat([df_target, df_other]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Use the function\n",
    "df = bias_type_column(df, bias_factor=0.4, target_type='center')\n",
    "\n",
    "# ---------------- Step 3: Preprocess text fields ----------------\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower()\n",
    "    elif isinstance(text, list):\n",
    "        return ' '.join([str(item).lower() for item in text])\n",
    "    else:\n",
    "        return str(text).lower()\n",
    "\n",
    "text_cols = ['text', 'topic', 'article', 'biased_words']\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(preprocess_text)\n",
    "    else:\n",
    "        df[col] = \"\"\n",
    "\n",
    "df[\"combined_input\"] = df[\"text\"].fillna(\"\") + \" \" + df[\"topic\"].fillna(\"\") + \" \" + df[\"article\"].fillna(\"\") + \" \" + df[\"biased_words\"].fillna(\"\")\n",
    "\n",
    "# ---------------- Step 4: Label Mapping ----------------\n",
    "label_map = {\n",
    "    \"Extreme_Left\": 0, \"slightly left\": 1, \"left\": 2,\n",
    "    \"center\": 3, \"right\": 4, \"slightly right\": 5, \"extreme right\": 6\n",
    "}\n",
    "\n",
    "df[\"labels\"] = df[\"type\"].map(label_map)\n",
    "df = df.dropna(subset=[\"combined_input\", \"labels\"])\n",
    "\n",
    "# ---------------- Step 5: Load Model & Tokenizer ----------------\n",
    "try:\n",
    "    model_name = \"launch/POLITICS\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "    politics_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "except:\n",
    "    model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    politics_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ---------------- Step 6: Rule + Model Bias Classification ----------------\n",
    "def predict_ideology_7class(text, model_pipeline, threshold=0.8):\n",
    "    result = model_pipeline(text, truncation=True, max_length=512)\n",
    "    base_label = result[0]['label'].lower()\n",
    "    confidence = result[0]['score']\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    bias_terms = {\n",
    "        \"Extreme_Left\": [\"revolution\", \"abolish capitalism\", \"anti-fascist\", \"socialist revolution\", \"wealth redistribution\", \"marxism\", \"anti-police\", \"defund the police\"],\n",
    "        \"slightly left\": [\"affordable college\", \"student loan relief\", \"raise minimum wage\", \"climate action\", \"universal childcare\", \"gun reform\"],\n",
    "        \"left\": [\"progressive\", \"social justice\", \"climate change\", \"lgbtq rights\", \"income inequality\", \"reproductive rights\", \"green new deal\"],\n",
    "        \"center\": [\"bipartisan\", \"moderate\", \"balanced budget\", \"common ground\", \"centrist\", \"neutral\", \"compromise\"],\n",
    "        \"right\": [\"free market\", \"border security\", \"second amendment\", \"traditional values\", \"small government\", \"lower taxes\", \"school choice\"],\n",
    "        \"slightly right\": [\"family values\", \"tough on crime\", \"economic freedom\", \"energy independence\"],\n",
    "        \"extreme right\": [\"nationalism\", \"patriot movement\", \"anti-immigration\", \"build the wall\", \"deep state\", \"cultural marxism\", \"gun rights absolutism\"]\n",
    "    }\n",
    "\n",
    "    liberal_terms = [\n",
    "        \"progressive\", \"universal healthcare\", \"climate change\", \"social justice\",\n",
    "        \"lgbtq rights\", \"income inequality\", \"reproductive rights\", \"green new deal\",\n",
    "        \"student loan relief\", \"gun reform\", \"affordable college\"\n",
    "    ]\n",
    "    conservative_terms = [\n",
    "        \"free market\", \"border security\", \"second amendment\", \"traditional values\",\n",
    "        \"small government\", \"lower taxes\", \"pro life\", \"religious freedom\",\n",
    "        \"family values\", \"tough on crime\", \"economic freedom\", \"energy independence\"\n",
    "    ]\n",
    "\n",
    "    category_counts = {label: sum(term in text_lower for term in terms) for label, terms in bias_terms.items()}\n",
    "    top_label = max(category_counts, key=category_counts.get)\n",
    "    top_count = category_counts[top_label]\n",
    "\n",
    "    liberal_count = sum(term in text_lower for term in liberal_terms)\n",
    "    conservative_count = sum(term in text_lower for term in conservative_terms)\n",
    "    margin = abs(liberal_count - conservative_count)\n",
    "\n",
    "    if top_count == 0 or confidence < threshold:\n",
    "        if liberal_count > conservative_count:\n",
    "            if margin >= 4: return \"Extreme_Left\"\n",
    "            elif margin == 3: return \"left\"\n",
    "            else: return \"slightly left\"\n",
    "        elif conservative_count > liberal_count:\n",
    "            if margin >= 4: return \"extreme right\"\n",
    "            elif margin == 3: return \"right\"\n",
    "            else: return \"slightly right\"\n",
    "        else:\n",
    "            return \"center\"\n",
    "    else:\n",
    "        return top_label\n",
    "\n",
    "# Apply adjusted logic\n",
    "df[\"adjusted_type\"] = df[\"combined_input\"].apply(lambda text: predict_ideology_7class(text, politics_pipe))\n",
    "df[\"adjusted_label\"] = df[\"adjusted_type\"].map(label_map)\n",
    "df = df.dropna(subset=[\"adjusted_label\"])\n",
    "\n",
    "# ---------------- Step 7: Prepare Dataset ----------------\n",
    "# First check category counts\n",
    "category_counts = df['adjusted_label'].value_counts()\n",
    "categories_with_few_samples = category_counts[category_counts < 2].index.tolist()\n",
    "\n",
    "# Conditional stratified or random split\n",
    "if categories_with_few_samples:\n",
    "    print(f\"Warning: Categories {categories_with_few_samples} have less than 2 samples. Removing stratification.\")\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)  # No stratify\n",
    "else:\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df['adjusted_label'], random_state=42)\n",
    "\n",
    "# Convert to Huggingface Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(eval_df.reset_index(drop=True))\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)\n",
    "\n",
    "## Rename columns\n",
    "if \"labels\" in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.remove_columns(\"labels\")\n",
    "train_dataset = train_dataset.rename_column(\"adjusted_label\", \"labels\")\n",
    "\n",
    "if \"labels\" in eval_dataset.column_names:\n",
    "    eval_dataset = eval_dataset.remove_columns(\"labels\")\n",
    "eval_dataset = eval_dataset.rename_column(\"adjusted_label\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# ---------------- Step 8: Training Setup ----------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/internship_vaali_infotech/results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/internship_vaali_infotech/logs\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ---------------- Step 9: Train and Save ----------------\n",
    "trainer.train()\n",
    "model.save_pretrained(\"/finetuned_politics_7class\")\n",
    "tokenizer.save_pretrained(\"//finetuned_politics_7class\")\n",
    "\n",
    "# ---------------- Step 10: Predict and Save ----------------\n",
    "full_dataset = Dataset.from_pandas(df)\n",
    "full_dataset = full_dataset.map(tokenize, batched=True)\n",
    "full_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "predictions = trainer.predict(full_dataset)\n",
    "predicted_classes = torch.argmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "df[\"predicted_bias_category\"] = [reverse_label_map[label] for label in predicted_classes]\n",
    "\n",
    "df.to_csv(\"/predicted_bias_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

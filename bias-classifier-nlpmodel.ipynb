{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Bias classifier \n","* Pre-trained models\n","* bert-base-uncased,facebook/roberta-hate-speech-dynabench-r4-target,T5"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-24T10:04:34.915265Z","iopub.status.busy":"2025-04-24T10:04:34.914650Z","iopub.status.idle":"2025-04-24T10:04:57.473709Z","shell.execute_reply":"2025-04-24T10:04:57.473082Z","shell.execute_reply.started":"2025-04-24T10:04:34.915240Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-24 10:04:43.937145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745489084.112784      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745489084.162116      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer\n","from transformers import default_data_collator\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","from datasets import Dataset\n","import pandas as pd\n","from sklearn.utils import resample\n","from functools import partial\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-25T05:15:18.893012Z","iopub.status.busy":"2025-04-25T05:15:18.892860Z","iopub.status.idle":"2025-04-25T05:15:22.898843Z","shell.execute_reply":"2025-04-25T05:15:22.898069Z","shell.execute_reply.started":"2025-04-25T05:15:18.892997Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA available: True\n","Device count: 1\n","Current device: 0\n","Device name: Tesla P100-PCIE-16GB\n"]}],"source":["import torch\n","print(\"CUDA available:\", torch.cuda.is_available())\n","print(\"Device count:\", torch.cuda.device_count())\n","print(\"Current device:\", torch.cuda.current_device())\n","print(\"Device name:\", torch.cuda.get_device_name(0))\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-25T05:16:17.576402Z","iopub.status.busy":"2025-04-25T05:16:17.575785Z","iopub.status.idle":"2025-04-25T05:16:17.581145Z","shell.execute_reply":"2025-04-25T05:16:17.580457Z","shell.execute_reply.started":"2025-04-25T05:16:17.576376Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T10:19:53.271575Z","iopub.status.busy":"2025-04-23T10:19:53.270951Z","iopub.status.idle":"2025-04-23T10:22:28.196902Z","shell.execute_reply":"2025-04-23T10:22:28.195925Z","shell.execute_reply.started":"2025-04-23T10:19:53.271543Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Collecting torch\n","  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.2.0 (from torch)\n","  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Collecting fsspec (from torch)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, transformers\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.1.0\n","    Uninstalling triton-3.1.0:\n","      Successfully uninstalled triton-3.1.0\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n","    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.9.90\n","    Uninstalling nvidia-curand-cu12-10.3.9.90:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n","    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n","    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n","    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n","    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu124\n","    Uninstalling torch-2.5.1+cu124:\n","      Successfully uninstalled torch-2.5.1+cu124\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.51.1\n","    Uninstalling transformers-4.51.1:\n","      Successfully uninstalled transformers-4.51.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n","bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n","fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n","torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n","pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n","pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0 transformers-4.51.3 triton-3.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["\n","pip install --upgrade transformers torch datasets"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-04-23T10:45:03.416360Z","iopub.status.busy":"2025-04-23T10:45:03.415705Z","iopub.status.idle":"2025-04-23T13:31:59.360196Z","shell.execute_reply":"2025-04-23T13:31:59.359416Z","shell.execute_reply.started":"2025-04-23T10:45:03.416338Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2674823439f049adb4cbf829172be953","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/52884 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8f54e76293f406badaef23a73eabc6b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13221 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='19833' max='19833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [19833/19833 2:28:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.016400</td>\n","      <td>0.010606</td>\n","      <td>0.998563</td>\n","      <td>0.998563</td>\n","      <td>0.998566</td>\n","      <td>0.998563</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.005900</td>\n","      <td>0.002697</td>\n","      <td>0.999471</td>\n","      <td>0.999471</td>\n","      <td>0.999471</td>\n","      <td>0.999471</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.001000</td>\n","      <td>0.001106</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"346ea1652f46455c8aa9f8921ee10b12","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/66105 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import sys\n","import torch\n","import numpy as np\n","import pandas as pd\n","import cupy as cp\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.utils import resample\n","from datasets import Dataset\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator\n",")\n","from functools import partial\n","import logging\n","\n","# Set up logging to debug recursion\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Increase recursion limit cautiously\n","sys.setrecursionlimit(1500)\n","\n","# Disable W&B\n","os.environ[\"WANDB_MODE\"] = \"disabled\"\n","\n","# Constants\n","MODEL_NAME = \"bert-base-uncased\"\n","LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n","REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n","OUTPUT_DIR = \"/kaggle/working/results\"\n","LOGGING_DIR = \"/kaggle/working/logs\"\n","RESULTS_PATH = \"/kaggle/working/predictions.csv\"\n","SAVE_PATH = OUTPUT_DIR\n","\n","# Ensure directories exist\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(LOGGING_DIR, exist_ok=True)\n","os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# ---------------- Text Processing Functions ----------------\n","def preprocess_text(text):\n","    return ' '.join(str(t).lower() for t in text if isinstance(t, str)) if isinstance(text, list) else str(text).lower()\n","\n","def combine_text(df, text_cols):\n","    for col in text_cols:\n","        df[col] = df[col].apply(preprocess_text) if col in df else \"\"\n","    df[\"combined_input\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n","    return df\n","\n","def tokenize(batch, tokenizer):\n","    return tokenizer(batch[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# ---------------- Rebalancing Function ----------------\n","def rebalance_three_categories_fixed_length(df, bias_factor=0.35, target_type='center', final_length=None):\n","    if final_length is None:\n","        final_length = len(df)\n","\n","    df_target = df[df['type'] == target_type]\n","    df_other = df[df['type'] != target_type]\n","\n","    n_target = int(final_length * bias_factor)\n","    n_other_total = final_length - n_target\n","    n_per_other = n_other_total // (df['type'].nunique() - 1)\n","\n","    if len(df_target) >= n_target:\n","        df_target_balanced = resample(df_target, n_samples=n_target, replace=False, random_state=42)\n","    else:\n","        df_target_balanced = resample(df_target, n_samples=n_target, replace=True, random_state=42)\n","\n","    other_types = [t for t in df['type'].unique() if t != target_type]\n","    df_others_balanced = []\n","    \n","    for t in other_types:\n","        df_t = df[df['type'] == t]\n","        if len(df_t) >= n_per_other:\n","            df_resampled = resample(df_t, n_samples=n_per_other, replace=False, random_state=42)\n","        else:\n","            df_resampled = resample(df_t, n_samples=n_per_other, replace=True, random_state=42)\n","        df_others_balanced.append(df_resampled)\n","\n","    df_final = pd.concat([df_target_balanced] + df_others_balanced)\n","    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n","    return df_final\n","\n","# ---------------- Load & Rebalance ----------------\n","def load_and_rebalance(file_path):\n","    logger.info(\"Loading and rebalancing data\")\n","    df = pd.read_excel(file_path, engine='openpyxl')\n","    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n","    \n","    logger.info(\"Original distribution:\")\n","    logger.info(df[\"type\"].value_counts())\n","    \n","    df_balanced = rebalance_three_categories_fixed_length(\n","        df,\n","        bias_factor=0.35,\n","        target_type='center',\n","        final_length=len(df['type'])\n","    )\n","    \n","    logger.info(\"\\nBalanced distribution:\")\n","    logger.info(df_balanced[\"type\"].value_counts())\n","    \n","    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n","    df_processed = combine_text(df_balanced, text_cols)\n","    df_processed[\"label\"] = df_processed[\"type\"].map(LABEL_MAP)\n","    \n","    return df_processed\n","\n","# ---------------- Metrics ----------------\n","def compute_metrics(eval_pred):\n","    logger.info(\"Computing metrics\")\n","    logits, labels = eval_pred\n","    logits = cp.asarray(logits)\n","    preds = cp.argmax(logits, axis=1).get()\n","    labels = labels\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# ---------------- Main Pipeline ----------------\n","def main(file_path):\n","    logger.info(\"Starting main pipeline\")\n","    # Load and prepare data\n","    df = load_and_rebalance(file_path)\n","    \n","    # Train-Test Split\n","    logger.info(\"Splitting data\")\n","    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","    \n","    # Convert to Dataset objects\n","    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n","    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n","\n","    # Load Tokenizer and Model\n","    logger.info(f\"Loading tokenizer and model: {MODEL_NAME}\")\n","    try:\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_NAME,\n","            num_labels=3,\n","            ignore_mismatched_sizes=True\n","        )\n","    except Exception as e:\n","        logger.error(f\"Model {MODEL_NAME} not found, using fallback model. Error: {e}\")\n","        fallback = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","        tokenizer = AutoTokenizer.from_pretrained(fallback)\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            fallback,\n","            num_labels=3,\n","            ignore_mismatched_sizes=True\n","        )\n","\n","    # Move model to device\n","    model.to(device)\n","\n","    # Tokenization\n","    logger.info(\"Tokenizing datasets\")\n","    tokenize_fn = partial(tokenize, tokenizer=tokenizer)\n","    train_ds = train_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    eval_ds = eval_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    \n","    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","    # Training Arguments\n","    logger.info(\"Setting up training arguments\")\n","    training_args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        logging_dir=LOGGING_DIR,\n","        do_train=True,\n","        do_eval=True,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        num_train_epochs=3,\n","        learning_rate=2e-5,\n","        weight_decay=0.01,\n","        remove_unused_columns=False,\n","        logging_steps=50,\n","        fp16=torch.cuda.is_available(),\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"accuracy\",\n","        greater_is_better=True,\n","        report_to=\"none\",\n","        no_cuda=not torch.cuda.is_available()\n","    )\n","\n","    # Initialize Trainer\n","    logger.info(\"Initializing Trainer\")\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=eval_ds,\n","        processing_class=tokenizer,\n","        compute_metrics=compute_metrics,\n","        data_collator=default_data_collator\n","    )\n","\n","    # Train\n","    logger.info(\"Starting training\")\n","    trainer.train()\n","\n","    # Save model and tokenizer\n","    logger.info(\"Saving model and tokenizer\")\n","    model.save_pretrained(SAVE_PATH)\n","    tokenizer.save_pretrained(SAVE_PATH)\n","    logger.info(f\"Model and tokenizer saved to {SAVE_PATH}\")\n","\n","    # Prediction\n","    logger.info(\"Making predictions\")\n","    full_ds = Dataset.from_pandas(df[[\"combined_input\"]].reset_index(drop=True))\n","    full_ds = full_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n","    \n","    preds = trainer.predict(full_ds)\n","    pred_logits = cp.asarray(preds.predictions)\n","    pred_labels = cp.argmax(pred_logits, axis=1).get()\n","    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in pred_labels]\n","    df.to_csv(RESULTS_PATH, index=False)\n","    logger.info(f\"Predictions saved to {RESULTS_PATH}\")\n","\n","if __name__ == \"__main__\":\n","    input_file_path = \"/kaggle/input/combined-data/combined_data.xlsx\"\n","    main(input_file_path)"]},{"cell_type":"markdown","metadata":{},"source":["### New Sampling techniques"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-24T05:38:06.419955Z","iopub.status.busy":"2025-04-24T05:38:06.419525Z","iopub.status.idle":"2025-04-24T08:26:37.512847Z","shell.execute_reply":"2025-04-24T08:26:37.512143Z","shell.execute_reply.started":"2025-04-24T05:38:06.419929Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-24 05:38:18.667569: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745473098.842827      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745473098.897872      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1198945aad6b4b7abd82e2140d4dda5a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e55b9ffd31e4dbca36036ffbe7b99c0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"444baaacc3f04ebb9a992c52fc232264","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ce42d096cd497990456dcff5d63460","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a66502408404f19af82a58ce9b2bc14","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4591775573534efc87cc0c2a44a77b90","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/52884 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8386556ab4594254aa7d76e382500541","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13221 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='19833' max='19833' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [19833/19833 2:29:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.001600</td>\n","      <td>0.008459</td>\n","      <td>0.998033</td>\n","      <td>0.998033</td>\n","      <td>0.998039</td>\n","      <td>0.998033</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0.001799</td>\n","      <td>0.999622</td>\n","      <td>0.999622</td>\n","      <td>0.999622</td>\n","      <td>0.999622</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000000</td>\n","      <td>0.000734</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","      <td>0.999924</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2523cedeeef64aa5baced4ef1cec67c9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/66105 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import sys\n","import torch\n","import numpy as np\n","import pandas as pd\n","import cupy as cp\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.utils import resample\n","from datasets import Dataset\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator\n",")\n","from functools import partial\n","import logging\n","\n","# Set up logging to debug recursion\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Increase recursion limit cautiously\n","sys.setrecursionlimit(1500)\n","\n","# Disable W&B\n","os.environ[\"WANDB_MODE\"] = \"disabled\"\n","\n","# Constants\n","MODEL_NAME = \"bert-base-uncased\"\n","LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n","REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n","OUTPUT_DIR = \"/kaggle/working/results\"\n","LOGGING_DIR = \"/kaggle/working/logs\"\n","RESULTS_PATH = \"/kaggle/working/predictions.csv\"\n","SAVE_PATH = OUTPUT_DIR\n","\n","# Ensure directories exist\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(LOGGING_DIR, exist_ok=True)\n","os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# ---------------- Text Processing Functions ----------------\n","def preprocess_text(text):\n","    return ' '.join(str(t).lower() for t in text if isinstance(t, str)) if isinstance(text, list) else str(text).lower()\n","\n","def combine_text(df, text_cols):\n","    for col in text_cols:\n","        df[col] = df[col].apply(preprocess_text) if col in df else \"\"\n","    df[\"combined_input\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n","    return df\n","\n","def tokenize(batch, tokenizer):\n","    return tokenizer(batch[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# ---------------- Rebalancing Function ----------------\n","def rebalance_three_categories_fixed_length(df, final_length=None):\n","    if final_length is None:\n","        final_length = len(df)\n","\n","    # Calculate equal number of samples per class\n","    unique_classes = df['type'].nunique()\n","    n_per_class = final_length // unique_classes\n","\n","    # Adjust final_length to ensure it's divisible by number of classes\n","    final_length = n_per_class * unique_classes\n","\n","    # Resample each class to have exactly n_per_class samples\n","    balanced_dfs = []\n","    for class_type in df['type'].unique():\n","        df_class = df[df['type'] == class_type]\n","        if len(df_class) >= n_per_class:\n","            df_resampled = resample(df_class, n_samples=n_per_class, replace=False, random_state=42)\n","        else:\n","            df_resampled = resample(df_class, n_samples=n_per_class, replace=True, random_state=42)\n","        balanced_dfs.append(df_resampled)\n","\n","    # Combine and shuffle the balanced dataset\n","    df_final = pd.concat(balanced_dfs)\n","    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n","    return df_final\n","\n","# ---------------- Load & Rebalance ----------------\n","def load_and_rebalance(file_path):\n","    logger.info(\"Loading and rebalancing data\")\n","    df = pd.read_excel(file_path, engine='openpyxl')\n","    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n","    \n","    logger.info(\"Original distribution:\")\n","    logger.info(df[\"type\"].value_counts())\n","    \n","    df_balanced = rebalance_three_categories_fixed_length(\n","        df,\n","        final_length=len(df)\n","    )\n","    \n","    logger.info(\"\\nBalanced distribution:\")\n","    logger.info(df_balanced[\"type\"].value_counts())\n","    \n","    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n","    df_processed = combine_text(df_balanced, text_cols)\n","    df_processed[\"label\"] = df_processed[\"type\"].map(LABEL_MAP)\n","    \n","    return df_processed\n","\n","# ---------------- Metrics ----------------\n","def compute_metrics(eval_pred):\n","    logger.info(\"Computing metrics\")\n","    logits, labels = eval_pred\n","    logits = cp.asarray(logits)\n","    preds = cp.argmax(logits, axis=1).get()\n","    labels = labels\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# ---------------- Main Pipeline ----------------\n","def main(file_path):\n","    logger.info(\"Starting main pipeline\")\n","    # Load and prepare data\n","    df = load_and_rebalance(file_path)\n","    \n","    # Train-Test Split\n","    logger.info(\"Splitting data\")\n","    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","    \n","    # Convert to Dataset objects\n","    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n","    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n","\n","    # Load Tokenizer and Model\n","    logger.info(f\"Loading tokenizer and model: {MODEL_NAME}\")\n","    try:\n","        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            MODEL_NAME,\n","            num_labels=3,\n","            ignore_mismatched_sizes=True\n","        )\n","    except Exception as e:\n","        logger.error(f\"Model {MODEL_NAME} not found, using fallback model. Error: {e}\")\n","        fallback = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n","        tokenizer = AutoTokenizer.from_pretrained(fallback)\n","        model = AutoModelForSequenceClassification.from_pretrained(\n","            fallback,\n","            num_labels=3,\n","            ignore_mismatched_sizes=True\n","        )\n","\n","    # Move model to device\n","    model.to(device)\n","\n","    # Tokenization\n","    logger.info(\"Tokenizing datasets\")\n","    tokenize_fn = partial(tokenize, tokenizer=tokenizer)\n","    train_ds = train_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    eval_ds = eval_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    \n","    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","    # Training Arguments\n","    logger.info(\"Setting up training arguments\")\n","    training_args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        logging_dir=LOGGING_DIR,\n","        do_train=True,\n","        do_eval=True,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        num_train_epochs=3,\n","        learning_rate=2e-5,\n","        weight_decay=0.01,\n","        remove_unused_columns=False,\n","        logging_steps=50,\n","        fp16=torch.cuda.is_available(),\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"accuracy\",\n","        greater_is_better=True,\n","        report_to=\"none\",\n","        no_cuda=not torch.cuda.is_available()\n","    )\n","\n","    # Initialize Trainer\n","    logger.info(\"Initializing Trainer\")\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=eval_ds,\n","        processing_class=tokenizer,\n","        compute_metrics=compute_metrics,\n","        data_collator=default_data_collator\n","    )\n","\n","    # Train\n","    logger.info(\"Starting training\")\n","    trainer.train()\n","\n","    # Save model and tokenizer\n","    logger.info(\"Saving model and tokenizer\")\n","    model.save_pretrained(SAVE_PATH)\n","    tokenizer.save_pretrained(SAVE_PATH)\n","    logger.info(f\"Model and tokenizer saved to {SAVE_PATH}\")\n","\n","    # Prediction\n","    logger.info(\"Making predictions\")\n","    full_ds = Dataset.from_pandas(df[[\"combined_input\"]].reset_index(drop=True))\n","    full_ds = full_ds.map(tokenize_fn, batched=True, batch_size=1000)\n","    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n","    \n","    preds = trainer.predict(full_ds)\n","    pred_logits = cp.asarray(preds.predictions)\n","    pred_labels = cp.argmax(pred_logits, axis=1).get()\n","    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in pred_labels]\n","    df.to_csv(RESULTS_PATH, index=False)\n","    logger.info(f\"Predictions saved to {RESULTS_PATH}\")\n","\n","if __name__ == \"__main__\":\n","    input_file_path = \"/kaggle/input/combined-data/combined_data.xlsx\"\n","    main(input_file_path)"]},{"cell_type":"markdown","metadata":{},"source":["### T5\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-25T05:16:42.229007Z","iopub.status.busy":"2025-04-25T05:16:42.228490Z","iopub.status.idle":"2025-04-25T06:22:05.823714Z","shell.execute_reply":"2025-04-25T06:22:05.822512Z","shell.execute_reply.started":"2025-04-25T05:16:42.228982Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-25 05:16:52.294907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1745558212.482353      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1745558212.534018      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"026b2e82193e493b8fd99a9e6fca67dc","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1819c20cdcf940c3bb529a9e0ca107d2","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf98e3975a2c43929e94a1c5602bd5b3","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c06ddb15932e49079ea0d76eec90ac47","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a206bbbdf22e4f9382185d2cf8ba4bd3","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5ff204faf2c475cadd35cff1597fe34","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/52884 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8e3a545bc7442a280af561c7f6f90c2","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/13221 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1654' max='4956' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1654/4956 1:02:57 < 2:05:49, 0.44 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='268' max='1653' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 268/1653 01:10 < 06:07, 3.77 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.15 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.17 GiB is free. Process 2694 has 14.72 GiB memory in use. Of the allocated memory 10.85 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/180123065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/combined-data/combined_data.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/180123065.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Train and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2662\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3096\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3097\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4373\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4375\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.15 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.17 GiB is free. Process 2694 has 14.72 GiB memory in use. Of the allocated memory 10.85 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["import os\n","import sys\n","import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from sklearn.utils import resample\n","from datasets import Dataset\n","from transformers import (\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    default_data_collator\n",")\n","import logging\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Constants\n","MODEL_NAME = \"google-t5/t5-base\"  # Use \"Salesforce/codet5-base\" for code-related tasks\n","LABELS = [\"left\", \"center\", \"right\"]\n","LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n","REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n","OUTPUT_DIR = \"/kaggle/working/results\"\n","LOGGING_DIR = \"/kaggle/working/logs\"\n","RESULTS_PATH = \"/kaggle/working/predictions.csv\"\n","SAVE_PATH = OUTPUT_DIR\n","\n","# Ensure directories exist\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","os.makedirs(LOGGING_DIR, exist_ok=True)\n","os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# ================== TEXT PROCESSING ==================\n","def preprocess_text(text):\n","    if isinstance(text, list):\n","        return ' '.join(str(t).lower() for t in text if isinstance(t, str))\n","    return str(text).lower()\n","\n","def combine_text(df, text_cols):\n","    df[text_cols] = df[text_cols].fillna(\"\").astype(str)\n","    for col in text_cols:\n","        df[col] = df[col].apply(preprocess_text)\n","    df[\"combined_input\"] = df[text_cols].agg(\" \".join, axis=1)\n","    return df\n","\n","# ================== DATA REBALANCING ==================\n","def rebalance_three_categories_fixed_length(df, bias_factor=0.35, target_type='center', final_length=None):\n","    if final_length is None:\n","        final_length = len(df)\n","\n","    df_target = df[df['type'] == target_type]\n","    df_other = df[df['type'] != target_type]\n","\n","    n_target = int(final_length * bias_factor)\n","    n_other_total = final_length - n_target\n","    n_per_other = n_other_total // (df['type'].nunique() - 1)\n","\n","    df_target_balanced = resample(\n","        df_target,\n","        n_samples=n_target,\n","        replace=len(df_target) < n_target,\n","        random_state=42\n","    )\n","\n","    other_types = [t for t in df['type'].unique() if t != target_type]\n","    df_others_balanced = []\n","    \n","    for t in other_types:\n","        df_t = df[df['type'] == t]\n","        df_resampled = resample(\n","            df_t,\n","            n_samples=n_per_other,\n","            replace=len(df_t) < n_per_other,\n","            random_state=42\n","        )\n","        df_others_balanced.append(df_resampled)\n","\n","    df_final = pd.concat([df_target_balanced] + df_others_balanced)\n","    return df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# ================== TOKENIZATION ==================\n","def tokenize(batch, tokenizer):\n","    inputs = tokenizer(\n","        batch[\"combined_input\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=512,\n","        return_tensors=\"pt\"\n","    )\n","    if \"label\" in batch:\n","        labels = tokenizer(\n","            batch[\"label\"],\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=10,\n","            return_tensors=\"pt\"\n","        )\n","        inputs[\"labels\"] = labels[\"input_ids\"]\n","    return inputs\n","\n","# ================== METRICS ==================\n","def compute_metrics(eval_pred):\n","    predictions, label_ids = eval_pred\n","    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","    preds = [LABEL_MAP.get(p, -1) for p in decoded_preds if p in LABEL_MAP]\n","    true_labels = [LABEL_MAP.get(l, -1) for l in decoded_labels if l in LABEL_MAP]\n","    valid_pairs = [(p, t) for p, t in zip(preds, true_labels) if p != -1 and t != -1]\n","    if not valid_pairs:\n","        return {\"accuracy\": 0.0, \"f1\": 0.0, \"precision\": 0.0, \"recall\": 0.0}\n","    preds, true_labels = zip(*valid_pairs)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n","    acc = accuracy_score(true_labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# ================== MAIN PIPELINE ==================\n","def main(file_path):\n","    logger.info(\"Starting pipeline\")\n","    \n","    # Load and preprocess data\n","    df = pd.read_excel(file_path, engine='openpyxl')\n","    df = df[df[\"type\"].isin(LABELS)]\n","    df_balanced = rebalance_three_categories_fixed_length(df)\n","    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n","    df_processed = combine_text(df_balanced, text_cols)\n","    df_processed[\"label\"] = df_processed[\"type\"]\n","    \n","    # Train-test split\n","    train_df, eval_df = train_test_split(df_processed, test_size=0.2, stratify=df_processed[\"label\"], random_state=42)\n","    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]])\n","    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]])\n","    \n","    # Initialize tokenizer and model\n","    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","    model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n","    \n","    # Tokenize datasets\n","    tokenize_fn = lambda batch: tokenize(batch, tokenizer)\n","    train_ds = train_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=4)\n","    eval_ds = eval_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=4)\n","    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","    \n","    # Training configuration\n","    training_args = TrainingArguments(\n","        output_dir=OUTPUT_DIR,\n","        logging_dir=LOGGING_DIR,\n","        do_train=True,\n","        do_eval=True,\n","        per_device_train_batch_size=8,\n","        per_device_eval_batch_size=8,\n","        num_train_epochs=3,\n","        learning_rate=2e-5,\n","        weight_decay=0.01,\n","        remove_unused_columns=False,\n","        logging_steps=50,\n","        fp16=torch.cuda.is_available(),\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"accuracy\",\n","        greater_is_better=True,\n","        report_to=\"none\",\n","        no_cuda=not torch.cuda.is_available(),\n","        gradient_accumulation_steps=4\n","      )\n","    \n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=eval_ds,\n","        compute_metrics=compute_metrics,\n","        data_collator=default_data_collator\n","    )\n","    \n","    # Train and save\n","    trainer.train()\n","    model.save_pretrained(SAVE_PATH)\n","    tokenizer.save_pretrained(SAVE_PATH)\n","    \n","    # Generate predictions with constrained decoding\n","    full_ds = Dataset.from_pandas(df_processed[[\"combined_input\"]])\n","    full_ds = full_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=4)\n","    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n","    \n","    # Constrained decoding to ensure predictions are in LABELS\n","    force_words_ids = tokenizer(LABELS, add_special_tokens=False).input_ids\n","    with torch.no_grad():\n","        preds = model.generate(\n","            input_ids=full_ds[\"input_ids\"].to(device),\n","            attention_mask=full_ds[\"attention_mask\"].to(device),\n","            max_length=10,\n","            force_words_ids=force_words_ids\n","        )\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    df_processed[\"predicted_type\"] = decoded_preds\n","    df_processed[\"predicted_label\"] = df_processed[\"predicted_type\"].map(LABEL_MAP)\n","    df_processed[[\"combined_input\", \"type\", \"predicted_type\", \"predicted_label\"]].to_csv(RESULTS_PATH, index=False)\n","    logger.info(f\"Results saved to {RESULTS_PATH}\")\n","\n","if __name__ == \"__main__\":\n","    main(\"/kaggle/input/combined-data/combined_data.xlsx\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7220897,"sourceId":11514710,"sourceType":"datasetVersion"},{"datasetId":7229856,"sourceId":11527440,"sourceType":"datasetVersion"}],"dockerImageVersionId":31013,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "* Combining many datsets and balancing the data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.12.4 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Dir = \"./final_data\"\n",
    "os.makedirs(Output_Dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All excel files combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavi/Library/Python/3.9/lib/python/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_base.py:778\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[1;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m    777\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[0;32m--> 778\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_openpyxl.py:616\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[0;32m--> 616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         converted_row\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_openpyxl.py:616\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[0;32m--> 616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         converted_row\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/excel/_openpyxl.py:590\u001b[0m, in \u001b[0;36mOpenpyxlReader._convert_cell\u001b[0;34m(self, cell)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_convert_cell\u001b[39m(\u001b[38;5;28mself\u001b[39m, cell) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Scalar:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcell\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcell\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    591\u001b[0m         TYPE_ERROR,\n\u001b[1;32m    592\u001b[0m         TYPE_NUMERIC,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# compat with xlrd\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## combine all files to single file for trainging model\n",
    "folder_path = \"./data_biasingmodel\"\n",
    "all_data = []\n",
    "\n",
    "# Loop through all .xlsx files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {filename}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save to a single Excel or CSV file\n",
    "combined_df.to_excel(os.path.join(folder_path, \"combined_data.xlsx\"), index=False)\n",
    "print(f\"Combined {len(all_data)} files with total {len(combined_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting missingno\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: numpy in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from missingno) (2.0.2)\n",
      "Collecting matplotlib (from missingno)\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_10_12_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from missingno) (1.13.1)\n",
      "Collecting seaborn (from missingno)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->missingno)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->missingno)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->missingno)\n",
      "  Downloading fonttools-4.57.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->missingno)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from matplotlib->missingno) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from matplotlib->missingno) (11.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->missingno)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from matplotlib->missingno) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib->missingno)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from seaborn->missingno) (2.2.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->missingno) (3.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn->missingno) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from pandas>=1.2->seaborn->missingno) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.15.0)\n",
      "Downloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-macosx_10_12_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.0-cp39-cp39-macosx_10_9_x86_64.whl (265 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp39-cp39-macosx_10_9_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-macosx_10_9_x86_64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib, seaborn, missingno\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 missingno-0.5.2 pyparsing-3.2.3 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xl = pd.read_excel(\"./data_biasingmodel/combined_data.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>label_bias</th>\n",
       "      <th>label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words</th>\n",
       "      <th>Label_bias_0-1</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>df_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>https://www.alternet.org/2019/01/here-are-5-of...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>['crisis']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>https://thefederalist.com/2020/03/11/woman-who...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>abortion</td>\n",
       "      <td>right</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Republican president assumed he was helpin...</td>\n",
       "      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>environment</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>In Barack Obama’s first term, the administrati...</td>\n",
       "      <td>['rejects', 'happy', 'assumed']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The explosion of the Hispanic population has l...</td>\n",
       "      <td>https://www.breitbart.com/politics/2015/02/26/...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>student-debt</td>\n",
       "      <td>right</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>No agreement</td>\n",
       "      <td>Republicans should stop fighting amnesty, Pres...</td>\n",
       "      <td>['explosion']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  YouTube is making clear there will be no “birt...   \n",
       "1  So while there may be a humanitarian crisis dr...   \n",
       "2  Looking around the United States, there is nev...   \n",
       "3  The Republican president assumed he was helpin...   \n",
       "4  The explosion of the Hispanic population has l...   \n",
       "\n",
       "                                           news_link      outlet  \\\n",
       "0  https://eu.usatoday.com/story/tech/2020/02/03/...   usa-today   \n",
       "1  https://www.alternet.org/2019/01/here-are-5-of...    alternet   \n",
       "2  https://thefederalist.com/2020/03/11/woman-who...  federalist   \n",
       "3  http://www.msnbc.com/rachel-maddow-show/auto-i...       msnbc   \n",
       "4  https://www.breitbart.com/politics/2015/02/26/...   breitbart   \n",
       "\n",
       "            topic    type  group_id  num_sent label_bias  \\\n",
       "0  elections-2020  center       1.0       1.0     Biased   \n",
       "1     immigration    left       1.0       1.0     Biased   \n",
       "2        abortion   right       1.0       1.0     Biased   \n",
       "3     environment    left       1.0       1.0     Biased   \n",
       "4    student-debt   right       1.0       1.0     Biased   \n",
       "\n",
       "                           label_opinion  \\\n",
       "0  Somewhat factual but also opinionated   \n",
       "1             Expresses writer’s opinion   \n",
       "2  Somewhat factual but also opinionated   \n",
       "3             Expresses writer’s opinion   \n",
       "4                           No agreement   \n",
       "\n",
       "                                             article  \\\n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1  Speaking to the country for the first time fro...   \n",
       "2  The left has a thing for taking babies hostage...   \n",
       "3  In Barack Obama’s first term, the administrati...   \n",
       "4  Republicans should stop fighting amnesty, Pres...   \n",
       "\n",
       "                                        biased_words  Label_bias_0-1  \\\n",
       "0                          ['belated', 'birtherism']             NaN   \n",
       "1                                         ['crisis']             NaN   \n",
       "2  ['killing', 'never', 'developing', 'humans', '...             NaN   \n",
       "3                    ['rejects', 'happy', 'assumed']             NaN   \n",
       "4                                      ['explosion']             NaN   \n",
       "\n",
       "   annotator_id  df_id  \n",
       "0           NaN    NaN  \n",
       "1           NaN    NaN  \n",
       "2           NaN    NaN  \n",
       "3           NaN    NaN  \n",
       "4           NaN    NaN  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl_new= df_xl[['topic', 'text', 'biased_words', 'article', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>biased_words</th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elections-2020</td>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>immigration</td>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>['crisis']</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>environment</td>\n",
       "      <td>The Republican president assumed he was helpin...</td>\n",
       "      <td>['rejects', 'happy', 'assumed']</td>\n",
       "      <td>In Barack Obama’s first term, the administrati...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>student-debt</td>\n",
       "      <td>The explosion of the Hispanic population has l...</td>\n",
       "      <td>['explosion']</td>\n",
       "      <td>Republicans should stop fighting amnesty, Pres...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            topic                                               text  \\\n",
       "0  elections-2020  YouTube is making clear there will be no “birt...   \n",
       "1     immigration  So while there may be a humanitarian crisis dr...   \n",
       "2        abortion  Looking around the United States, there is nev...   \n",
       "3     environment  The Republican president assumed he was helpin...   \n",
       "4    student-debt  The explosion of the Hispanic population has l...   \n",
       "\n",
       "                                        biased_words  \\\n",
       "0                          ['belated', 'birtherism']   \n",
       "1                                         ['crisis']   \n",
       "2  ['killing', 'never', 'developing', 'humans', '...   \n",
       "3                    ['rejects', 'happy', 'assumed']   \n",
       "4                                      ['explosion']   \n",
       "\n",
       "                                             article    type  \n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...  center  \n",
       "1  Speaking to the country for the first time fro...    left  \n",
       "2  The left has a thing for taking babies hostage...   right  \n",
       "3  In Barack Obama’s first term, the administrati...    left  \n",
       "4  Republicans should stop fighting amnesty, Pres...   right  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfxl_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "left      25764\n",
      "right     25734\n",
      "center    14608\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dfxl_new['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center' 'left' 'right' nan]\n"
     ]
    }
   ],
   "source": [
    "print(dfxl_new['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic            1762\n",
       "text                2\n",
       "biased_words    38512\n",
       "article         74866\n",
       "type            11960\n",
       "dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfxl_new.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All json files combined and saved to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "json_dir = \"./llama3_8b\"\n",
    "output_dir = \"bias_classifier/llama3_8b\"\n",
    "all_data = []\n",
    "\n",
    "# Load and preprocess data\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        with open(file_path, 'r') as f:\n",
    "            try:\n",
    "                file_data = json.load(f)\n",
    "                all_data.extend(file_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# --- Normalize 'analysis' field ---\n",
    "if 'analysis' in df.columns:\n",
    "    analysis_df = pd.json_normalize(df['analysis'])\n",
    "    df = pd.concat([df.drop(columns=['analysis']), analysis_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>bias_category</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>supporting_examples</th>\n",
       "      <th>additional_analysis</th>\n",
       "      <th>raw_response</th>\n",
       "      <th>model</th>\n",
       "      <th>analyzed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>https://indianexpress.com/article/technology/t...</td>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>[Nandagopal Rajan, Cdata, Var Template_Content...</td>\n",
       "      <td>2025-04-15 08:04:48+05:30</td>\n",
       "      <td>[air, macbook, really, m4with, version, need, ...</td>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>2025-04-15T17:56:47.989815</td>\n",
       "      <td>Center</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>BIAS CATEGORY: Center\\n\\nREASONING:\\n\\nThe art...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2025-04-15T22:21:48.742441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>https://indianexpress.com/article/india/bihar-...</td>\n",
       "      <td>Even MLCs are not safe: Bihar RJD leader says ...</td>\n",
       "      <td>Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...</td>\n",
       "      <td>[Himanshu Harsh, Cdata, Var Template_Content, ...</td>\n",
       "      <td>2025-04-15 15:32:09+05:30</td>\n",
       "      <td>[arrested, mentioned, number, information, sho...</td>\n",
       "      <td>Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...</td>\n",
       "      <td>2025-04-15T17:56:31.289229</td>\n",
       "      <td>Center</td>\n",
       "      <td>The article presents a factual account of an a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>BIAS CATEGORY: Center\\n\\nREASONING: The articl...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2025-04-15T22:21:51.469832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>https://indianexpress.com/article/cities/delhi...</td>\n",
       "      <td>Delhi News Live Updates: Capital braces for he...</td>\n",
       "      <td>Delhi News Live Updates: Visitors at the India...</td>\n",
       "      <td>[Cdata, Var Template_Content, Sso_Login_Box, X...</td>\n",
       "      <td>2025-04-15 16:37:01+05:30</td>\n",
       "      <td>[updates, temperature, live, issues, 2025, ass...</td>\n",
       "      <td>Delhi News Live Updates: Visitors at the India...</td>\n",
       "      <td>2025-04-15T17:56:53.078721</td>\n",
       "      <td>Center</td>\n",
       "      <td>The article presents a mix of neutral and fact...</td>\n",
       "      <td></td>\n",
       "      <td>Overall, the tone of the article is neutral an...</td>\n",
       "      <td>BIAS CATEGORY: Center\\nREASONING: The article ...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2025-04-15T22:21:55.058438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>https://indianexpress.com/article/india/suprem...</td>\n",
       "      <td>‘One has to be careful’: Supreme Court objects...</td>\n",
       "      <td>The Supreme Court Tuesday criticised the Allah...</td>\n",
       "      <td>[Cdata, Var Template_Content, Sso_Login_Box, X...</td>\n",
       "      <td>2025-04-15 16:04:27+05:30</td>\n",
       "      <td>[court, trouble, objects, rape, high, careful,...</td>\n",
       "      <td>The Supreme Court Tuesday criticised the Allah...</td>\n",
       "      <td>2025-04-15T17:56:57.528312</td>\n",
       "      <td>Center</td>\n",
       "      <td>The article presents a neutral and objective a...</td>\n",
       "      <td></td>\n",
       "      <td>The article does not appear to promote a speci...</td>\n",
       "      <td>BIAS CATEGORY: Center\\n\\nREASONING: The articl...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2025-04-15T22:22:03.496738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>https://indianexpress.com/article/business/ind...</td>\n",
       "      <td>India’s retail inflation slips to over 5-year ...</td>\n",
       "      <td>India’s retail inflation slipped to a more-tha...</td>\n",
       "      <td>[Cdata, Var Template_Content, Sso_Login_Box, X...</td>\n",
       "      <td>2025-04-15 16:57:01+05:30</td>\n",
       "      <td>[cuts, prices, slips, low, opens, inflation, r...</td>\n",
       "      <td>India’s retail inflation slipped to a more-tha...</td>\n",
       "      <td>2025-04-15T17:56:58.267002</td>\n",
       "      <td>Center</td>\n",
       "      <td>The article presents a factual report on India...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>BIAS CATEGORY: Center\\n\\nREASONING: The articl...</td>\n",
       "      <td>llama3:8b</td>\n",
       "      <td>2025-04-15T22:22:06.094239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                                url  \\\n",
       "0  The Indian Express  https://indianexpress.com/article/technology/t...   \n",
       "1  The Indian Express  https://indianexpress.com/article/india/bihar-...   \n",
       "2  The Indian Express  https://indianexpress.com/article/cities/delhi...   \n",
       "3  The Indian Express  https://indianexpress.com/article/india/suprem...   \n",
       "4  The Indian Express  https://indianexpress.com/article/business/ind...   \n",
       "\n",
       "                                               title  \\\n",
       "0  You really don’t need a more powerful computer...   \n",
       "1  Even MLCs are not safe: Bihar RJD leader says ...   \n",
       "2  Delhi News Live Updates: Capital braces for he...   \n",
       "3  ‘One has to be careful’: Supreme Court objects...   \n",
       "4  India’s retail inflation slips to over 5-year ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  You really don’t need a more powerful computer...   \n",
       "1  Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...   \n",
       "2  Delhi News Live Updates: Visitors at the India...   \n",
       "3  The Supreme Court Tuesday criticised the Allah...   \n",
       "4  India’s retail inflation slipped to a more-tha...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Nandagopal Rajan, Cdata, Var Template_Content...   \n",
       "1  [Himanshu Harsh, Cdata, Var Template_Content, ...   \n",
       "2  [Cdata, Var Template_Content, Sso_Login_Box, X...   \n",
       "3  [Cdata, Var Template_Content, Sso_Login_Box, X...   \n",
       "4  [Cdata, Var Template_Content, Sso_Login_Box, X...   \n",
       "\n",
       "                publish_date  \\\n",
       "0  2025-04-15 08:04:48+05:30   \n",
       "1  2025-04-15 15:32:09+05:30   \n",
       "2  2025-04-15 16:37:01+05:30   \n",
       "3  2025-04-15 16:04:27+05:30   \n",
       "4  2025-04-15 16:57:01+05:30   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [air, macbook, really, m4with, version, need, ...   \n",
       "1  [arrested, mentioned, number, information, sho...   \n",
       "2  [updates, temperature, live, issues, 2025, ass...   \n",
       "3  [court, trouble, objects, rape, high, careful,...   \n",
       "4  [cuts, prices, slips, low, opens, inflation, r...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  You really don’t need a more powerful computer...   \n",
       "1  Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...   \n",
       "2  Delhi News Live Updates: Visitors at the India...   \n",
       "3  The Supreme Court Tuesday criticised the Allah...   \n",
       "4  India’s retail inflation slipped to a more-tha...   \n",
       "\n",
       "                   scraped_at bias_category  \\\n",
       "0  2025-04-15T17:56:47.989815        Center   \n",
       "1  2025-04-15T17:56:31.289229        Center   \n",
       "2  2025-04-15T17:56:53.078721        Center   \n",
       "3  2025-04-15T17:56:57.528312        Center   \n",
       "4  2025-04-15T17:56:58.267002        Center   \n",
       "\n",
       "                                           reasoning supporting_examples  \\\n",
       "0                                                                          \n",
       "1  The article presents a factual account of an a...                       \n",
       "2  The article presents a mix of neutral and fact...                       \n",
       "3  The article presents a neutral and objective a...                       \n",
       "4  The article presents a factual report on India...                       \n",
       "\n",
       "                                 additional_analysis  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  Overall, the tone of the article is neutral an...   \n",
       "3  The article does not appear to promote a speci...   \n",
       "4                                                      \n",
       "\n",
       "                                        raw_response      model  \\\n",
       "0  BIAS CATEGORY: Center\\n\\nREASONING:\\n\\nThe art...  llama3:8b   \n",
       "1  BIAS CATEGORY: Center\\n\\nREASONING: The articl...  llama3:8b   \n",
       "2  BIAS CATEGORY: Center\\nREASONING: The article ...  llama3:8b   \n",
       "3  BIAS CATEGORY: Center\\n\\nREASONING: The articl...  llama3:8b   \n",
       "4  BIAS CATEGORY: Center\\n\\nREASONING: The articl...  llama3:8b   \n",
       "\n",
       "                  analyzed_at  \n",
       "0  2025-04-15T22:21:48.742441  \n",
       "1  2025-04-15T22:21:51.469832  \n",
       "2  2025-04-15T22:21:55.058438  \n",
       "3  2025-04-15T22:22:03.496738  \n",
       "4  2025-04-15T22:22:06.094239  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source', 'url', 'title', 'text', 'authors', 'publish_date', 'keywords',\n",
       "       'summary', 'scraped_at', 'bias_category', 'reasoning',\n",
       "       'supporting_examples', 'additional_analysis', 'raw_response', 'model',\n",
       "       'analyzed_at'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename the relevant columns\n",
    "df = df.rename(columns={\n",
    "    'title': 'topic',\n",
    "    'text': 'text',\n",
    "    'keywords': 'biased_words',\n",
    "    'summary': 'article',\n",
    "    'bias_category': 'type'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['topic', 'text', 'biased_words', 'article', 'type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>biased_words</th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>[air, macbook, really, m4with, version, need, ...</td>\n",
       "      <td>You really don’t need a more powerful computer...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even MLCs are not safe: Bihar RJD leader says ...</td>\n",
       "      <td>Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...</td>\n",
       "      <td>[arrested, mentioned, number, information, sho...</td>\n",
       "      <td>Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi News Live Updates: Capital braces for he...</td>\n",
       "      <td>Delhi News Live Updates: Visitors at the India...</td>\n",
       "      <td>[updates, temperature, live, issues, 2025, ass...</td>\n",
       "      <td>Delhi News Live Updates: Visitors at the India...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘One has to be careful’: Supreme Court objects...</td>\n",
       "      <td>The Supreme Court Tuesday criticised the Allah...</td>\n",
       "      <td>[court, trouble, objects, rape, high, careful,...</td>\n",
       "      <td>The Supreme Court Tuesday criticised the Allah...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India’s retail inflation slips to over 5-year ...</td>\n",
       "      <td>India’s retail inflation slipped to a more-tha...</td>\n",
       "      <td>[cuts, prices, slips, low, opens, inflation, r...</td>\n",
       "      <td>India’s retail inflation slipped to a more-tha...</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic  \\\n",
       "0  You really don’t need a more powerful computer...   \n",
       "1  Even MLCs are not safe: Bihar RJD leader says ...   \n",
       "2  Delhi News Live Updates: Capital braces for he...   \n",
       "3  ‘One has to be careful’: Supreme Court objects...   \n",
       "4  India’s retail inflation slips to over 5-year ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  You really don’t need a more powerful computer...   \n",
       "1  Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...   \n",
       "2  Delhi News Live Updates: Visitors at the India...   \n",
       "3  The Supreme Court Tuesday criticised the Allah...   \n",
       "4  India’s retail inflation slipped to a more-tha...   \n",
       "\n",
       "                                        biased_words  \\\n",
       "0  [air, macbook, really, m4with, version, need, ...   \n",
       "1  [arrested, mentioned, number, information, sho...   \n",
       "2  [updates, temperature, live, issues, 2025, ass...   \n",
       "3  [court, trouble, objects, rape, high, careful,...   \n",
       "4  [cuts, prices, slips, low, opens, inflation, r...   \n",
       "\n",
       "                                             article    type  \n",
       "0  You really don’t need a more powerful computer...  Center  \n",
       "1  Rashtriya Janata Dal’s Mohammad Shoaib, a Memb...  Center  \n",
       "2  Delhi News Live Updates: Visitors at the India...  Center  \n",
       "3  The Supreme Court Tuesday criticised the Allah...  Center  \n",
       "4  India’s retail inflation slipped to a more-tha...  Center  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/1529717764.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfxl_new['text'] = dfxl_new['text'].fillna('')  # Fill with empty string\n",
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/1529717764.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfxl_new[col] = imputed_values\n",
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/1529717764.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfxl_new[col] = imputed_values\n",
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/1529717764.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfxl_new[col] = imputed_values\n",
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/1529717764.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfxl_new[col] = imputed_values\n"
     ]
    }
   ],
   "source": [
    "dfxl_new['text'] = dfxl_new['text'].fillna('')  # Fill with empty string\n",
    "\n",
    "# Define imputation strategies for each column\n",
    "imputation_strategies = {\n",
    "    'topic': 'most_frequent',  # Categorical column\n",
    "    'biased_words': 'constant',  # Will specify fill_value\n",
    "    'article': 'constant',      # Will specify fill_value\n",
    "    'type': 'most_frequent'    # Categorical column\n",
    "}\n",
    "\n",
    "# Apply imputation to each column\n",
    "for col, strategy in imputation_strategies.items():\n",
    "    if strategy == 'constant':\n",
    "        # For list-like/text columns, fill with empty string representation\n",
    "        imputer = SimpleImputer(strategy=strategy, fill_value='[]' if col == 'biased_words' else '')\n",
    "    else:\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "    \n",
    "    # Reshape to 2D array as required by sklearn\n",
    "    imputed_values = imputer.fit_transform(dfxl_new[[col]]).ravel()\n",
    "    dfxl_new[col] = imputed_values\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Missing values after imputation:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic           0\n",
      "text            0\n",
      "biased_words    0\n",
      "article         0\n",
      "type            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dfxl_new.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Json files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Center' 'Slightly Left' 'Slightly Right' 'Right' 'Extreme Right'\n",
      " 'Center/Slightly Right' 'Slightly Center' 'Center/Left' 'Center-Left'\n",
      " 'Center-Right' 'Center/Slightly Left' 'Slightly left']\n"
     ]
    }
   ],
   "source": [
    "print(df_new['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mapping = {\n",
    "    # Center\n",
    "    'Center': 'center',\n",
    "    'Center/Slightly Right': 'center',\n",
    "    'Slightly Center': 'center',\n",
    "    'Center-Left': 'center',\n",
    "    'Center-Right': 'center',\n",
    "    'Center/Slightly Left': 'center',\n",
    "    \n",
    "    # Left\n",
    "    'Slightly Left': 'left',\n",
    "    'Center/Left': 'left',\n",
    "    'Slightly left': 'left',  # Note: lowercase 'left' corrected\n",
    "    \n",
    "    # Right\n",
    "    'Slightly Right': 'right',\n",
    "    'Right': 'right',\n",
    "    'Extreme Right': 'right'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zl/014zkbv57bb_wpycm8r859740000gn/T/ipykernel_17664/4232125881.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_new['type'] = df_new['type'].map(bias_mapping)\n"
     ]
    }
   ],
   "source": [
    "df_new['type'] = df_new['type'].map(bias_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center' 'left' 'right']\n"
     ]
    }
   ],
   "source": [
    "print(df_new['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "center    3497\n",
      "right      366\n",
      "left       333\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_new['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic           0\n",
       "text            0\n",
       "biased_words    0\n",
       "article         0\n",
       "type            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat both the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (82262, 5)\n",
      "After dropping NaN: (82262, 5)\n",
      "Saved to data.xlsx\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([dfxl_new, df_new], ignore_index=True)\n",
    "\n",
    "# Drop ALL rows with ANY NaN values\n",
    "combined_df_cleaned = combined_df.dropna()\n",
    "\n",
    "# Save to Excel\n",
    "combined_df_cleaned.to_excel(\"./final_data/data.xlsx\", index=False)\n",
    "\n",
    "print(f\"Original shape: {combined_df.shape}\")\n",
    "print(f\"After dropping NaN: {combined_df_cleaned.shape}\")\n",
    "print(\"Saved to data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl = pd.read_excel(\"./final_data/data.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>biased_words</th>\n",
       "      <th>article</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elections-2020</td>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>immigration</td>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>['crisis']</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>environment</td>\n",
       "      <td>The Republican president assumed he was helpin...</td>\n",
       "      <td>['rejects', 'happy', 'assumed']</td>\n",
       "      <td>In Barack Obama’s first term, the administrati...</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>student-debt</td>\n",
       "      <td>The explosion of the Hispanic population has l...</td>\n",
       "      <td>['explosion']</td>\n",
       "      <td>Republicans should stop fighting amnesty, Pres...</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            topic                                               text  \\\n",
       "0  elections-2020  YouTube is making clear there will be no “birt...   \n",
       "1     immigration  So while there may be a humanitarian crisis dr...   \n",
       "2        abortion  Looking around the United States, there is nev...   \n",
       "3     environment  The Republican president assumed he was helpin...   \n",
       "4    student-debt  The explosion of the Hispanic population has l...   \n",
       "\n",
       "                                        biased_words  \\\n",
       "0                          ['belated', 'birtherism']   \n",
       "1                                         ['crisis']   \n",
       "2  ['killing', 'never', 'developing', 'humans', '...   \n",
       "3                    ['rejects', 'happy', 'assumed']   \n",
       "4                                      ['explosion']   \n",
       "\n",
       "                                             article    type  \n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...  center  \n",
       "1  Speaking to the country for the first time fro...    left  \n",
       "2  The left has a thing for taking babies hostage...   right  \n",
       "3  In Barack Obama’s first term, the administrati...    left  \n",
       "4  Republicans should stop fighting amnesty, Pres...   right  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center' 'left' 'right']\n"
     ]
    }
   ],
   "source": [
    "print(df_fl['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "left      38057\n",
      "right     26100\n",
      "center    18105\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_fl['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'text', 'biased_words', 'article', 'type'], dtype='object')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fl.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy is very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavi/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "center    38057\n",
      "left      38057\n",
      "right     38057\n",
      "Name: count, dtype: int64\n",
      "Balanced dataset saved as 'balanced_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace NaN values with empty strings for text columns\n",
    "text_columns = ['text', 'biased_words', 'article']\n",
    "df_fl[text_columns] = df_fl[text_columns].fillna('')\n",
    "\n",
    "# Step 1: Preprocess the features\n",
    "# Encode categorical column 'topic' using one-hot encoding\n",
    "topic_encoded = pd.get_dummies(df_fl['topic'], prefix='topic', sparse=True)\n",
    "\n",
    "# Vectorize text columns ('text', 'biased_words', 'article') using TF-IDF\n",
    "vectorizers = {}\n",
    "X_text_sparse = []\n",
    "for col in text_columns:\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # Limit features to avoid high dimensionality\n",
    "    X_col = vectorizer.fit_transform(df_fl[col])\n",
    "    vectorizers[col] = vectorizer\n",
    "    X_text_sparse.append(X_col)\n",
    "\n",
    "# Combine topic (one-hot encoded) and text features (TF-IDF)\n",
    "X_sparse = hstack([topic_encoded] + X_text_sparse, format='csr')\n",
    "\n",
    "# Step 2: Encode the target variable\n",
    "y = df_fl['type']\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 3: Apply SMOTE to balance the classes\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_sparse, y_encoded)\n",
    "\n",
    "# Step 4: Decode the target variable back to original labels\n",
    "y_resampled = label_encoder.inverse_transform(y_resampled)\n",
    "\n",
    "# Step 5: Reconstruct the original columns\n",
    "# Inverse transform 'topic' (one-hot encoded back to categorical)\n",
    "topic_columns = topic_encoded.columns\n",
    "topic_resampled = X_resampled[:, :len(topic_columns)].toarray()\n",
    "topic_labels = np.array([topic_columns[i].replace('topic_', '') for i in topic_resampled.argmax(axis=1)])\n",
    "\n",
    "# Inverse transform text columns (approximate from TF-IDF)\n",
    "df_resampled = pd.DataFrame()\n",
    "for col in text_columns:\n",
    "    # Get the TF-IDF features for this column\n",
    "    start_idx = len(topic_columns) + sum(1000 for c in text_columns if c < col)\n",
    "    end_idx = start_idx + 1000\n",
    "    X_col_resampled = X_resampled[:, start_idx:end_idx]\n",
    "    # Use the vectorizer to approximate the text (this is a simplification)\n",
    "    df_resampled[col] = [' '.join(vectorizers[col].get_feature_names_out()[X_col_resampled[i].nonzero()[1]]) \n",
    "                         if len(X_col_resampled[i].nonzero()[1]) > 0 else '' \n",
    "                         for i in range(X_resampled.shape[0])]\n",
    "\n",
    "# Add 'topic' and 'type' to the DataFrame\n",
    "df_resampled['topic'] = topic_labels\n",
    "df_resampled['type'] = y_resampled\n",
    "\n",
    "# Reorder columns to match the original\n",
    "df_resampled = df_resampled[['topic', 'text', 'biased_words', 'article', 'type']]\n",
    "\n",
    "# Step 6: Verify the class distribution\n",
    "print(df_resampled['type'].value_counts())\n",
    "\n",
    "# Step 7: Save the balanced dataset\n",
    "df_resampled.to_csv('./final_data/balanced_data.csv', index=False)\n",
    "print(\"Balanced dataset saved as 'balanced_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the code for balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "type\n",
      "left      38057\n",
      "right     26100\n",
      "center    18105\n",
      "Name: count, dtype: int64\n",
      "Balancing all classes to approximately 26100 samples\n",
      "Oversampling class 'center' from 18105 to 26100\n",
      "Undersampling class 'left' from 38057 to 26100\n",
      "Class 'right' already has 26100 samples (target: 26100)\n",
      "\n",
      "Final class distribution:\n",
      "type\n",
      "center    26100\n",
      "left      26100\n",
      "right     26100\n",
      "Name: count, dtype: int64\n",
      "Complete balanced dataset saved as 'complete_balanced_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN values only in text columns\n",
    "text_columns = ['text', 'biased_words', 'article']\n",
    "for col in text_columns:\n",
    "    if df_fl[col].isna().any():\n",
    "        df_fl[col] = df_fl[col].fillna('')\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(df_fl['type'].value_counts())\n",
    "\n",
    "# Method 1: Balanced dataset with text preservation\n",
    "# This approach balances classes while preserving text quality\n",
    "\n",
    "# Step 1: Determine target count for each class\n",
    "# Option A: Balance to median class size\n",
    "class_counts = df_fl['type'].value_counts()\n",
    "median_count = class_counts.median()\n",
    "target_count = int(median_count)\n",
    "\n",
    "# Option B: Balance to a specific size (uncomment to use)\n",
    "# target_count = 500  # Set your desired count here\n",
    "\n",
    "print(f\"Balancing all classes to approximately {target_count} samples\")\n",
    "\n",
    "# Step 2: Process each class separately to create a balanced dataset\n",
    "balanced_dfs = []\n",
    "\n",
    "for class_name in df_fl['type'].unique():\n",
    "    class_df = df_fl[df_fl['type'] == class_name].copy()\n",
    "    class_size = len(class_df)\n",
    "    \n",
    "    if class_size > target_count:\n",
    "        # Undersample this class\n",
    "        print(f\"Undersampling class '{class_name}' from {class_size} to {target_count}\")\n",
    "        # Randomly sample without replacement\n",
    "        sampled_df = class_df.sample(n=target_count, random_state=42)\n",
    "        balanced_dfs.append(sampled_df)\n",
    "    \n",
    "    elif class_size < target_count:\n",
    "        # Oversample this class\n",
    "        print(f\"Oversampling class '{class_name}' from {class_size} to {target_count}\")\n",
    "        \n",
    "        # Calculate how many samples to add\n",
    "        samples_needed = target_count - class_size\n",
    "        \n",
    "        # Randomly sample with replacement\n",
    "        # This preserves the text integrity better than synthetic samples\n",
    "        additional_samples = class_df.sample(n=samples_needed, replace=True, random_state=42)\n",
    "        \n",
    "        # Add small random noise to any numeric columns to avoid exact duplicates\n",
    "        # Skip this step for text columns to preserve text integrity\n",
    "        numeric_columns = class_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if numeric_columns:\n",
    "            for col in numeric_columns:\n",
    "                # Add very small random noise\n",
    "                noise_scale = class_df[col].std() * 0.01  # 1% of standard deviation\n",
    "                if noise_scale > 0:\n",
    "                    additional_samples[col] = additional_samples[col] + np.random.normal(0, noise_scale, len(additional_samples))\n",
    "        \n",
    "        # Combine original and additional samples\n",
    "        combined_df = pd.concat([class_df, additional_samples], ignore_index=True)\n",
    "        balanced_dfs.append(combined_df)\n",
    "    \n",
    "    else:\n",
    "        # This class already has the target count\n",
    "        print(f\"Class '{class_name}' already has {class_size} samples (target: {target_count})\")\n",
    "        balanced_dfs.append(class_df)\n",
    "\n",
    "# Step 3: Combine all balanced classes\n",
    "df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "# Verify final class distribution\n",
    "print(\"\\nFinal class distribution:\")\n",
    "print(df_balanced['type'].value_counts())\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the complete balanced dataset\n",
    "df_balanced.to_csv('./final_data/complete_balanced_data.csv', index=False)\n",
    "print(\"Complete balanced dataset saved as 'complete_balanced_data.csv'\")\n",
    "\n",
    "# Method 2: Alternative approach with more sophisticated SMOTE for text\n",
    "# This is a more advanced method if you prefer SMOTE-based balancing\n",
    "\n",
    "def create_advanced_balanced_dataset():\n",
    "    print(\"\\nAlternative Method: Advanced SMOTE with text preservation\")\n",
    "    \n",
    "    # Step 1: Separate features and target\n",
    "    X = df_fl.drop('type', axis=1)\n",
    "    y = df_fl['type']\n",
    "    \n",
    "    # Step 2: Extract non-text features for SMOTE\n",
    "    # For text datasets, we'll use categorical columns and any numeric features\n",
    "    categorical_cols = ['topic']  # Add any other categorical columns here\n",
    "    X_cat = pd.get_dummies(X[categorical_cols], sparse=False)\n",
    "    \n",
    "    # Step 3: Define sampling strategy to balance classes\n",
    "    class_counts = Counter(y)\n",
    "    sampling_strategy = {cls: target_count for cls in class_counts.keys()}\n",
    "    \n",
    "    # Step 4: Apply SMOTE on categorical features only\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_cat_resampled, y_resampled = smote.fit_resample(X_cat, y)\n",
    "    \n",
    "    # Step 5: Reconstruct full dataset with text\n",
    "    # Get indices of original samples used in resampling\n",
    "    original_indices = smote.sample_indices_\n",
    "    \n",
    "    # Create a dataframe for resampled data\n",
    "    df_resampled = pd.DataFrame(columns=df_fl.columns)\n",
    "    \n",
    "    # Add original samples\n",
    "    original_samples = df_fl.iloc[original_indices].copy()\n",
    "    df_resampled = pd.concat([df_resampled, original_samples], ignore_index=True)\n",
    "    \n",
    "    # Get synthetic sample indices\n",
    "    synthetic_indices = np.setdiff1d(np.arange(len(X_cat_resampled)), smote.sample_indices_)\n",
    "    \n",
    "    # For each synthetic sample, find nearest neighbor of same class and copy text\n",
    "    print(f\"Generating {len(synthetic_indices)} synthetic samples with preserved text...\")\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    for batch_start in range(0, len(synthetic_indices), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(synthetic_indices))\n",
    "        batch_indices = synthetic_indices[batch_start:batch_end]\n",
    "        \n",
    "        synthetic_samples = []\n",
    "        for idx in batch_indices:\n",
    "            # Get class of this synthetic sample\n",
    "            cls = y_resampled[idx]\n",
    "            \n",
    "            # Get one-hot encoded features\n",
    "            synthetic_features = X_cat_resampled[idx]\n",
    "            \n",
    "            # Find original samples of same class\n",
    "            cls_samples = df_fl[df_fl['type'] == cls]\n",
    "            cls_indices = cls_samples.index.tolist()\n",
    "            \n",
    "            if cls_indices:\n",
    "                # Get one-hot encoded features of original samples\n",
    "                cls_X_cat = pd.get_dummies(cls_samples[categorical_cols], sparse=False)\n",
    "                \n",
    "                # Find nearest neighbor\n",
    "                try:\n",
    "                    # Calculate distances\n",
    "                    distances = np.sum((cls_X_cat.values - synthetic_features) ** 2, axis=1)\n",
    "                    nearest_idx = cls_indices[np.argmin(distances)]\n",
    "                    \n",
    "                    # Get the nearest sample\n",
    "                    nearest_sample = df_fl.loc[nearest_idx].copy()\n",
    "                    \n",
    "                    # Add small variations to avoid exact duplicates\n",
    "                    # (but preserve text columns)\n",
    "                    for col in nearest_sample.index:\n",
    "                        if col not in text_columns and col != 'type' and isinstance(nearest_sample[col], (int, float)):\n",
    "                            # Add small random noise to numeric non-text columns\n",
    "                            col_std = df_fl[col].std()\n",
    "                            if col_std > 0:\n",
    "                                nearest_sample[col] += np.random.normal(0, col_std * 0.05)\n",
    "                    \n",
    "                    synthetic_samples.append(nearest_sample)\n",
    "                except:\n",
    "                    # Fallback: random sample from same class\n",
    "                    random_idx = np.random.choice(cls_indices)\n",
    "                    synthetic_samples.append(df_fl.loc[random_idx].copy())\n",
    "            else:\n",
    "                # Shouldn't happen if all classes have samples\n",
    "                print(f\"Warning: No samples found for class {cls}\")\n",
    "        \n",
    "        # Create dataframe batch and append\n",
    "        if synthetic_samples:\n",
    "            batch_df = pd.DataFrame(synthetic_samples)\n",
    "            df_resampled = pd.concat([df_resampled, batch_df], ignore_index=True)\n",
    "        \n",
    "        # Clean memory\n",
    "        gc.collect()\n",
    "    \n",
    "    # Verify final class distribution\n",
    "    print(\"\\nFinal class distribution (advanced method):\")\n",
    "    print(df_resampled['type'].value_counts())\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df_resampled = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Save the advanced balanced dataset\n",
    "    df_resampled.to_csv('./final_data/balanced_data.csv', index=False)\n",
    "    print(\"Advanced balanced dataset saved as 'balanced_data.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic           0\n",
       "text            0\n",
       "biased_words    0\n",
       "article         0\n",
       "type            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped files to ./final_data/bert_data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Define paths to your files\n",
    "files_to_zip = [\n",
    "    \"./train_bert_with_smote_data.py\",\n",
    "    \"./final_data/balanced_data.xlsx\"\n",
    "]\n",
    "\n",
    "# Output zip file path\n",
    "zip_path = \"./final_data/bert_data.zip\"\n",
    "\n",
    "# Create the zip file\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file, arcname=file.split(\"/\")[-1])\n",
    "\n",
    "print(f\"Zipped files to {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

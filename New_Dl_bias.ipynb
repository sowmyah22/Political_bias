{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning Model to detect political bias in news articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n",
    "    TrainingArguments, pipeline\n",
    ")\n",
    "#from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "from parellel import parallel_combine_text\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "* Load folder and read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>label_bias</th>\n",
       "      <th>label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words</th>\n",
       "      <th>Label_bias_0-1</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>df_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>['belated', 'birtherism']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>https://www.alternet.org/2019/01/here-are-5-of...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>['crisis']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>https://thefederalist.com/2020/03/11/woman-who...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>abortion</td>\n",
       "      <td>right</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>The left has a thing for taking babies hostage...</td>\n",
       "      <td>['killing', 'never', 'developing', 'humans', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Republican president assumed he was helpin...</td>\n",
       "      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>environment</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>In Barack Obama’s first term, the administrati...</td>\n",
       "      <td>['rejects', 'happy', 'assumed']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The explosion of the Hispanic population has l...</td>\n",
       "      <td>https://www.breitbart.com/politics/2015/02/26/...</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>student-debt</td>\n",
       "      <td>right</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>No agreement</td>\n",
       "      <td>Republicans should stop fighting amnesty, Pres...</td>\n",
       "      <td>['explosion']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  YouTube is making clear there will be no “birt...   \n",
       "1  So while there may be a humanitarian crisis dr...   \n",
       "2  Looking around the United States, there is nev...   \n",
       "3  The Republican president assumed he was helpin...   \n",
       "4  The explosion of the Hispanic population has l...   \n",
       "\n",
       "                                           news_link      outlet  \\\n",
       "0  https://eu.usatoday.com/story/tech/2020/02/03/...   usa-today   \n",
       "1  https://www.alternet.org/2019/01/here-are-5-of...    alternet   \n",
       "2  https://thefederalist.com/2020/03/11/woman-who...  federalist   \n",
       "3  http://www.msnbc.com/rachel-maddow-show/auto-i...       msnbc   \n",
       "4  https://www.breitbart.com/politics/2015/02/26/...   breitbart   \n",
       "\n",
       "            topic    type  group_id  num_sent label_bias  \\\n",
       "0  elections-2020  center       1.0       1.0     Biased   \n",
       "1     immigration    left       1.0       1.0     Biased   \n",
       "2        abortion   right       1.0       1.0     Biased   \n",
       "3     environment    left       1.0       1.0     Biased   \n",
       "4    student-debt   right       1.0       1.0     Biased   \n",
       "\n",
       "                           label_opinion  \\\n",
       "0  Somewhat factual but also opinionated   \n",
       "1             Expresses writer’s opinion   \n",
       "2  Somewhat factual but also opinionated   \n",
       "3             Expresses writer’s opinion   \n",
       "4                           No agreement   \n",
       "\n",
       "                                             article  \\\n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...   \n",
       "1  Speaking to the country for the first time fro...   \n",
       "2  The left has a thing for taking babies hostage...   \n",
       "3  In Barack Obama’s first term, the administrati...   \n",
       "4  Republicans should stop fighting amnesty, Pres...   \n",
       "\n",
       "                                        biased_words  Label_bias_0-1  \\\n",
       "0                          ['belated', 'birtherism']             NaN   \n",
       "1                                         ['crisis']             NaN   \n",
       "2  ['killing', 'never', 'developing', 'humans', '...             NaN   \n",
       "3                    ['rejects', 'happy', 'assumed']             NaN   \n",
       "4                                      ['explosion']             NaN   \n",
       "\n",
       "   annotator_id  df_id  \n",
       "0           NaN    NaN  \n",
       "1           NaN    NaN  \n",
       "2           NaN    NaN  \n",
       "3           NaN    NaN  \n",
       "4           NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## combine all files to single file for trainging model\n",
    "folder_path = \"./data_biasingmodel\"\n",
    "# ---------------- Config ----------------\n",
    "File_path = \"./data_biasingmodel/combined_data.xlsx\"\n",
    "LOGGING_DIR = \"./data_biasingmodel/logs\"\n",
    "RESULTS_PATH = \"./data_biasingmodel/predicted_bias_results.csv\"\n",
    "all_data = []\n",
    "\n",
    "# Loop through all .xlsx files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".xlsx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {filename}: {e}\")\n",
    "\n",
    "# Combine all the DataFrames\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save to a single Excel or CSV file\n",
    "combined_df.to_excel(os.path.join(folder_path, \"combined_data.xlsx\"), index=False)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    \"left\": 0,\n",
    "    \"center\": 1,\n",
    "    \"right\": 2\n",
    "}\n",
    "NUM_PROCESSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_combine_text(row_chunk, text_cols):\n",
    "    for col in text_cols:\n",
    "        row_chunk[col] = row_chunk[col].fillna(\"\")\n",
    "    row_chunk[\"text\"] = row_chunk[text_cols].agg(\" \".join, axis=1).str.strip()\n",
    "    return row_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      "type\n",
      "right     4965\n",
      "left      4953\n",
      "center    3464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced distribution:\n",
      "type\n",
      "center    4683\n",
      "left      4349\n",
      "right     4349\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-39:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'parallel_combine_text' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'parallel_combine_text' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "handle is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 300\u001b[0m\n\u001b[1;32m    297\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# # your code here\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# multiprocessing.set_start_method('spawn', force=True)\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[28], line 222\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Load and prepare data (parallel)\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_and_rebalance(file_path)\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Train-Test Split\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     train_df, eval_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 202\u001b[0m, in \u001b[0;36mload_and_rebalance\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m [df_balanced\u001b[38;5;241m.\u001b[39miloc[i:i \u001b[38;5;241m+\u001b[39m chunk_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_balanced), chunk_size)]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mNUM_PROCESSES) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 202\u001b[0m     processed_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    203\u001b[0m         partial(parallel_combine_text, text_cols\u001b[38;5;241m=\u001b[39mtext_cols),\n\u001b[1;32m    204\u001b[0m         chunks\n\u001b[1;32m    205\u001b[0m     ))\n\u001b[1;32m    207\u001b[0m df_processed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(processed_chunks)\n\u001b[1;32m    208\u001b[0m df_processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(LABEL_MAP)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/process.py:837\u001b[0m, in \u001b[0;36mProcessPoolExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunksize must be >= 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 837\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmap(partial(_process_chunk, fn),\n\u001b[1;32m    838\u001b[0m                       _get_chunks(\u001b[38;5;241m*\u001b[39miterables, chunksize\u001b[38;5;241m=\u001b[39mchunksize),\n\u001b[1;32m    839\u001b[0m                       timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chain_from_iterable_of_lists(results)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map\u001b[0;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    606\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    606\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 608\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/process.py:808\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_to_dynamically_spawn_children:\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adjust_process_count()\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_executor_manager_thread()\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/process.py:767\u001b[0m, in \u001b[0;36mProcessPoolExecutor._adjust_process_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m process_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes)\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_workers:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Assertion disabled as this codepath is also used to replace a\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# worker that unexpectedly dies, even when using the 'fork' start\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;66;03m# we know a thread is running (self._executor_manager_thread).\u001b[39;00m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m#assert self._safe_to_dynamically_spawn_children or not self._executor_manager_thread, 'https://github.com/python/cpython/issues/90622'\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/process.py:785\u001b[0m, in \u001b[0;36mProcessPoolExecutor._spawn_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_spawn_process\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    778\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mp_context\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    779\u001b[0m         target\u001b[38;5;241m=\u001b[39m_process_worker,\n\u001b[1;32m    780\u001b[0m         args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_queue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initargs,\n\u001b[1;32m    784\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_tasks_per_child))\n\u001b[0;32m--> 785\u001b[0m     p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes[p\u001b[38;5;241m.\u001b[39mpid] \u001b[38;5;241m=\u001b[39m p\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:984\u001b[0m, in \u001b[0;36mreduce_connection\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_connection\u001b[39m(conn):\n\u001b[0;32m--> 984\u001b[0m     df \u001b[38;5;241m=\u001b[39m reduction\u001b[38;5;241m.\u001b[39mDupFd(conn\u001b[38;5;241m.\u001b[39mfileno())\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rebuild_connection, (df, conn\u001b[38;5;241m.\u001b[39mreadable, conn\u001b[38;5;241m.\u001b[39mwritable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:171\u001b[0m, in \u001b[0;36m_ConnectionBase.fileno\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfileno\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"File descriptor or handle of the connection\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/connection.py:137\u001b[0m, in \u001b[0;36m_ConnectionBase._check_closed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_closed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: handle is closed"
     ]
    }
   ],
   "source": [
    "# ---------------- Custom Model with Additional Self-Attention ----------------\n",
    "class CustomModelWithAttention(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # Load base transformer model\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.config = self.base_model.config\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Additional self-attention layer\n",
    "        self.extra_attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.config.hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.extra_attention.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Base model forward pass\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Additional self-attention\n",
    "        attn_output, _ = self.extra_attention(\n",
    "            query=sequence_output,\n",
    "            key=sequence_output,\n",
    "            value=sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool() if attention_mask is not None else None\n",
    "        )\n",
    "        \n",
    "        # Residual connection + layer norm\n",
    "        sequence_output = self.layer_norm(sequence_output + attn_output)\n",
    "        \n",
    "        # Take [CLS] token representation for classification\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "# ---------------- Parallel Processing Functions ----------------\n",
    "def parallel_preprocess_text(text):\n",
    "    return ' '.join(str(t).lower() for t in text if isinstance(t, str)) if isinstance(text, list) else str(text).lower()\n",
    "\n",
    "def parallel_combine_text(df_chunk, text_cols):\n",
    "    for col in text_cols:\n",
    "        df_chunk[col] = df_chunk[col].apply(parallel_preprocess_text) if col in df_chunk else \"\"\n",
    "    df_chunk[\"combined_input\"] = df_chunk[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df_chunk\n",
    "\n",
    "def parallel_tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # ---------------- Load & Rebalance (Parallel) ----------------\n",
    "def load_and_rebalance(df_combined):\n",
    "    # Use the combined DataFrame directly\n",
    "    df = df_combined[df_combined[\"type\"].isin(LABEL_MAP.keys())].copy()\n",
    "    \n",
    "    print(\"Original distribution:\")\n",
    "    print(df[\"type\"].value_counts())\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    \n",
    "    # Calculate target counts based on your dataset size\n",
    "    total_samples = len(df)\n",
    "    # Option 2: Custom balancing \n",
    "    df_balanced = rebalance_three_categories_fixed_length(\n",
    "        df,\n",
    "        bias_factor=0.35,      # Center bias (35%)\n",
    "        target_type='center',   # Focus on center\n",
    "        final_length=len(df['type'])      # Larger final dataset size\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBalanced distribution:\")\n",
    "    print(df_balanced[\"type\"].value_counts())\n",
    "    print(f\"Total balanced samples: {len(df_balanced)}\")\n",
    "    \n",
    "    # Parallel text processing for large dataset\n",
    "    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n",
    "    chunk_size = len(df_balanced) // NUM_PROCESSES\n",
    "    \n",
    "    # Process in parallel chunks\n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        chunks = [df_balanced.iloc[i:i + chunk_size] for i in range(0, len(df_balanced), chunk_size)]\n",
    "        processed_chunks = list(executor.map(\n",
    "            partial(parallel_combine_text, text_cols=text_cols),\n",
    "            chunks\n",
    "        ))\n",
    "    \n",
    "    df_processed = pd.concat(processed_chunks)\n",
    "    df_processed[\"label\"] = df_processed[\"type\"].map(LABEL_MAP)\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "# ---------------- Main Pipeline ----------------\n",
    "def main():\n",
    "    # Load and prepare data (parallel)\n",
    "    df = load_and_rebalance(df_combined)\n",
    "    \n",
    "    # Rest of your pipeline remains the same...\n",
    "    # Train-Test Split\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "    \n",
    "    # Convert to Dataset objects\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "\n",
    "# ---------------- Rebalancing Function ----------------\n",
    "def rebalance_three_categories_fixed_length(df, bias_factor=0.35, target_type='center', final_length=None):\n",
    "    if final_length is None:\n",
    "        final_length = len(df)\n",
    "\n",
    "    # Separate target and other types\n",
    "    df_target = df[df['type'] == target_type]\n",
    "    df_other = df[df['type'] != target_type]\n",
    "\n",
    "    # Compute desired target count\n",
    "    n_target = int(final_length * bias_factor)\n",
    "    n_other_total = final_length - n_target\n",
    "    n_per_other = n_other_total // (df['type'].nunique() - 1)\n",
    "\n",
    "    # Resample the target type\n",
    "    if len(df_target) >= n_target:\n",
    "        df_target_balanced = resample(df_target, n_samples=n_target, replace=False, random_state=42)\n",
    "    else:\n",
    "        df_target_balanced = resample(df_target, n_samples=n_target, replace=True, random_state=42)\n",
    "\n",
    "    # Resample each other class\n",
    "    other_types = [t for t in df['type'].unique() if t != target_type]\n",
    "    df_others_balanced = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        futures = []\n",
    "        for t in other_types:\n",
    "            df_t = df[df['type'] == t]\n",
    "            futures.append(executor.submit(\n",
    "                resample, df_t, \n",
    "                n_samples=n_per_other, \n",
    "                replace=len(df_t) < n_per_other,\n",
    "                random_state=42\n",
    "            ))\n",
    "        \n",
    "        for future in futures:\n",
    "            df_others_balanced.append(future.result())\n",
    "\n",
    "    # Combine all and shuffle\n",
    "    df_final = pd.concat([df_target_balanced] + df_others_balanced)\n",
    "    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "# ---------------- Load & Rebalance (Parallel) ----------------\n",
    "def load_and_rebalance(file_path):\n",
    "    # Parallel CSV reading\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n",
    "    \n",
    "    print(\"Original distribution:\")\n",
    "    print(df[\"type\"].value_counts())\n",
    "    \n",
    "    # Parallel rebalancing\n",
    "    df_balanced = rebalance_three_categories_fixed_length(\n",
    "        df,\n",
    "        bias_factor=0.35,\n",
    "        target_type='center',\n",
    "        final_length=len(df['type'])\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBalanced distribution:\")\n",
    "    print(df_balanced[\"type\"].value_counts())\n",
    "    \n",
    "    # Parallel text processing\n",
    "    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n",
    "    chunk_size = len(df_balanced) // NUM_PROCESSES\n",
    "    chunks = [df_balanced.iloc[i:i + chunk_size] for i in range(0, len(df_balanced), chunk_size)]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        processed_chunks = list(executor.map(\n",
    "            partial(parallel_combine_text, text_cols=text_cols),\n",
    "            chunks\n",
    "        ))\n",
    "    \n",
    "    df_processed = pd.concat(processed_chunks)\n",
    "    df_processed[\"label\"] = df_processed[\"type\"].map(LABEL_MAP)\n",
    "    return df_processed\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ---------------- Main Pipeline ----------------\n",
    "def main():\n",
    "    # Load and prepare data (parallel)\n",
    "    df = load_and_rebalance(file_path)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "    \n",
    "    # Convert to Dataset objects\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "    # Load Tokenizer and Custom Model\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = CustomModelWithAttention(MODEL_NAME, num_labels=3)\n",
    "    except:\n",
    "        print(f\"Model {MODEL_NAME} not found, using fallback model\")\n",
    "        fallback = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
    "        model = CustomModelWithAttention(fallback, num_labels=3)\n",
    "\n",
    "    # Parallel tokenization\n",
    "    tokenize_fn = partial(parallel_tokenize, tokenizer=tokenizer)\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    eval_ds = eval_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    \n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    # Training arguments with multiprocessing\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        logging_dir=LOGGING_DIR,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers=NUM_PROCESSES,  # Parallel data loading\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # Train and save\n",
    "    trainer.train()\n",
    "    model.save_pretrained(folder_path)\n",
    "    tokenizer.save_pretrained(folder_path)\n",
    "\n",
    "    # Parallel prediction\n",
    "    full_ds = Dataset.from_pandas(df[[\"combined_input\"]].reset_index(drop=True))\n",
    "    full_ds = full_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    preds = trainer.predict(full_ds)\n",
    "    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()]\n",
    "    df.to_csv(RESULTS_PATH, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set multiprocessing start method for Unix systems\n",
    "    pool = Pool()\n",
    "    # # your code here\n",
    "    # multiprocessing.set_start_method('spawn', force=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "\n",
    "MODEL_NAME = \"launch/POLITICS\"  # fallback model will be handled\n",
    "LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n",
    "REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "\n",
    "# ---------------- Load & Rebalance ----------------\n",
    "def load_and_rebalance(data):\n",
    "    df = data\n",
    "    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n",
    "\n",
    "    # Rebalance to equal left, center, right\n",
    "    min_count = min(df[\"type\"].value_counts()[label] for label in LABEL_MAP)\n",
    "    df_list = [resample(df[df[\"type\"] == label], n_samples=min_count, random_state=42) for label in LABEL_MAP]\n",
    "    return pd.concat(df_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ---------------- Preprocessing ----------------\n",
    "def preprocess_text(text):\n",
    "    return ' '.join(str(t).lower() for t in text if isinstance(t, str)) if isinstance(text, list) else str(text).lower()\n",
    "\n",
    "def combine_text_columns(df, text_cols):\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].apply(preprocess_text) if col in df else \"\"\n",
    "    df[\"combined_input\"] = df[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------- Tokenization ----------------\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    return dataset.map(lambda x: tokenizer(x[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512), batched=True)\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ---------------- Main Pipeline ----------------\n",
    "def main():\n",
    "    df = load_and_rebalance(FILE_PATH)\n",
    "    df = combine_text_columns(df, [\"text\", \"topic\", \"article\", \"biased_words\"])\n",
    "    df[\"label\"] = df[\"type\"].map(LABEL_MAP)\n",
    "\n",
    "    # Train-Test Split\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "    # Load Model & Tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "    except:\n",
    "        fallback = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(fallback, num_labels=3)\n",
    "\n",
    "    train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "    eval_ds = tokenize_dataset(eval_ds, tokenizer)\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        logging_dir=LOGGING_DIR,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    full_ds = Dataset.from_pandas(df[[\"combined_input\"]].reset_index(drop=True))\n",
    "    full_ds = tokenize_dataset(full_ds, tokenizer)\n",
    "    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    preds = trainer.predict(full_ds)\n",
    "    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()]\n",
    "    df.to_csv(RESULTS_PATH, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer,\n",
    "    TrainingArguments, default_data_collator\n",
    ")\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "FILE_PATH = \"/content/drive/MyDrive/internship_vaali_infotech/final_labels_MBIC.xlsx\"\n",
    "MODEL_NAME = \"launch/POLITICS\"  # fallback model will be handled\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/internship_vaali_infotech/finetuned_politics_3class\"\n",
    "LOGGING_DIR = \"/content/drive/MyDrive/internship_vaali_infotech/logs\"\n",
    "RESULTS_PATH = \"/content/drive/MyDrive/internship_vaali_infotech/predicted_bias_results.csv\"\n",
    "LABEL_MAP = {\"left\": 0, \"center\": 1, \"right\": 2}\n",
    "REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
    "NUM_PROCESSES = multiprocessing.cpu_count()  # Use all available cores\n",
    "\n",
    "# ---------------- Custom Model with Additional Self-Attention ----------------\n",
    "class CustomModelWithAttention(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        # Load base transformer model\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.config = self.base_model.config\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Additional self-attention layer\n",
    "        self.extra_attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.config.hidden_size,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for name, param in self.extra_attention.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Base model forward pass\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        \n",
    "        # Additional self-attention\n",
    "        attn_output, _ = self.extra_attention(\n",
    "            query=sequence_output,\n",
    "            key=sequence_output,\n",
    "            value=sequence_output,\n",
    "            key_padding_mask=~attention_mask.bool() if attention_mask is not None else None\n",
    "        )\n",
    "        \n",
    "        # Residual connection + layer norm\n",
    "        sequence_output = self.layer_norm(sequence_output + attn_output)\n",
    "        \n",
    "        # Take [CLS] token representation for classification\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "# ---------------- Parallel Processing Functions ----------------\n",
    "def parallel_preprocess_text(text):\n",
    "    return ' '.join(str(t).lower() for t in text if isinstance(t, str)) if isinstance(text, list) else str(text).lower()\n",
    "\n",
    "def parallel_combine_text(df_chunk, text_cols):\n",
    "    for col in text_cols:\n",
    "        df_chunk[col] = df_chunk[col].apply(parallel_preprocess_text) if col in df_chunk else \"\"\n",
    "    df_chunk[\"combined_input\"] = df_chunk[text_cols].fillna(\"\").agg(\" \".join, axis=1)\n",
    "    return df_chunk\n",
    "\n",
    "def parallel_tokenize(batch, tokenizer):\n",
    "    return tokenizer(batch[\"combined_input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# ---------------- Rebalancing Function ----------------\n",
    "def rebalance_three_categories_fixed_length(df, bias_factor=0.35, target_type='center', final_length=None):\n",
    "    if final_length is None:\n",
    "        final_length = len(df)\n",
    "\n",
    "    # Separate target and other types\n",
    "    df_target = df[df['type'] == target_type]\n",
    "    df_other = df[df['type'] != target_type]\n",
    "\n",
    "    # Compute desired target count\n",
    "    n_target = int(final_length * bias_factor)\n",
    "    n_other_total = final_length - n_target\n",
    "    n_per_other = n_other_total // (df['type'].nunique() - 1)\n",
    "\n",
    "    # Resample the target type\n",
    "    if len(df_target) >= n_target:\n",
    "        df_target_balanced = resample(df_target, n_samples=n_target, replace=False, random_state=42)\n",
    "    else:\n",
    "        df_target_balanced = resample(df_target, n_samples=n_target, replace=True, random_state=42)\n",
    "\n",
    "    # Resample each other class\n",
    "    other_types = [t for t in df['type'].unique() if t != target_type]\n",
    "    df_others_balanced = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        futures = []\n",
    "        for t in other_types:\n",
    "            df_t = df[df['type'] == t]\n",
    "            futures.append(executor.submit(\n",
    "                resample, df_t, \n",
    "                n_samples=n_per_other, \n",
    "                replace=len(df_t) < n_per_other,\n",
    "                random_state=42\n",
    "            ))\n",
    "        \n",
    "        for future in futures:\n",
    "            df_others_balanced.append(future.result())\n",
    "\n",
    "    # Combine all and shuffle\n",
    "    df_final = pd.concat([df_target_balanced] + df_others_balanced)\n",
    "    df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df_final\n",
    "\n",
    "# ---------------- Load & Rebalance (Parallel) ----------------\n",
    "def load_and_rebalance(file_path):\n",
    "    # Parallel CSV reading\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    df = df[df[\"type\"].isin(LABEL_MAP.keys())]\n",
    "    \n",
    "    print(\"Original distribution:\")\n",
    "    print(df[\"type\"].value_counts())\n",
    "    \n",
    "    # Parallel rebalancing\n",
    "    df_balanced = rebalance_three_categories_fixed_length(\n",
    "        df,\n",
    "        bias_factor=0.35,\n",
    "        target_type='center',\n",
    "        final_length=1700\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBalanced distribution:\")\n",
    "    print(df_balanced[\"type\"].value_counts())\n",
    "    \n",
    "    # Parallel text processing\n",
    "    text_cols = [\"text\", \"topic\", \"article\", \"biased_words\"]\n",
    "    chunk_size = len(df_balanced) // NUM_PROCESSES\n",
    "    chunks = [df_balanced.iloc[i:i + chunk_size] for i in range(0, len(df_balanced), chunk_size)]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        processed_chunks = list(executor.map(\n",
    "            partial(parallel_combine_text, text_cols=text_cols),\n",
    "            chunks\n",
    "        ))\n",
    "    \n",
    "    df_processed = pd.concat(processed_chunks)\n",
    "    df_processed[\"label\"] = df_processed[\"type\"].map(LABEL_MAP)\n",
    "    return df_processed\n",
    "\n",
    "# ---------------- Metrics ----------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# ---------------- Main Pipeline ----------------\n",
    "def main():\n",
    "    # Load and prepare data (parallel)\n",
    "    df = load_and_rebalance(FILE_PATH)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "    \n",
    "    # Convert to Dataset objects\n",
    "    train_ds = Dataset.from_pandas(train_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "    eval_ds = Dataset.from_pandas(eval_df[[\"combined_input\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "    # Load Tokenizer and Custom Model\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = CustomModelWithAttention(MODEL_NAME, num_labels=3)\n",
    "    except:\n",
    "        print(f\"Model {MODEL_NAME} not found, using fallback model\")\n",
    "        fallback = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
    "        model = CustomModelWithAttention(fallback, num_labels=3)\n",
    "\n",
    "    # Parallel tokenization\n",
    "    tokenize_fn = partial(parallel_tokenize, tokenizer=tokenizer)\n",
    "    train_ds = train_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    eval_ds = eval_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    \n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    # Training arguments with multiprocessing\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        logging_dir=LOGGING_DIR,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers=NUM_PROCESSES,  # Parallel data loading\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU available\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # Train and save\n",
    "    trainer.train()\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Parallel prediction\n",
    "    full_ds = Dataset.from_pandas(df[[\"combined_input\"]].reset_index(drop=True))\n",
    "    full_ds = full_ds.map(tokenize_fn, batched=True, batch_size=1000, num_proc=NUM_PROCESSES)\n",
    "    full_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    preds = trainer.predict(full_ds)\n",
    "    df[\"predicted_bias_category\"] = [REVERSE_LABEL_MAP[i] for i in torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()]\n",
    "    df.to_csv(RESULTS_PATH, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set multiprocessing start method for Unix systems\n",
    "    multiprocessing.set_start_method('spawn', force=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pranavi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pranavi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./analyzed_articles/llama3_8b/the_indian_express_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/ndtv_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/the_hindu_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/news18_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/times_of_india_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/zee_news_augmented.json\n",
      "Saved: ./analyzed_articles/llama3_8b/india_today_augmented.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from copy import deepcopy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def synonym_replacement(sentence, n=2):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = words[:]\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            if synonym != random_word:\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def augment_article(article):\n",
    "    aug_article = deepcopy(article)\n",
    "    if \"text\" in article and article[\"text\"]:\n",
    "        aug_article[\"text\"] = synonym_replacement(article[\"text\"], n=3)\n",
    "    if \"summary\" in article and article[\"summary\"]:\n",
    "        aug_article[\"summary\"] = synonym_replacement(article[\"summary\"], n=2)\n",
    "    if \"analysis\" in article and \"reasoning\" in article[\"analysis\"]:\n",
    "        aug_article[\"analysis\"][\"reasoning\"] = synonym_replacement(article[\"analysis\"][\"reasoning\"], n=2)\n",
    "    aug_article[\"augmented\"] = True\n",
    "    return aug_article\n",
    "\n",
    "def process_file(input_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    augmented_data = [augment_article(item) for item in data]\n",
    "    final_data = data + augmented_data\n",
    "\n",
    "    output_path = input_path.replace(\"_analyzed.json\", \"_augmented.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "def main():\n",
    "    input_files = glob.glob(\"./analyzed_articles/llama3_8b/*_analyzed.json\")\n",
    "    for file in input_files:\n",
    "        process_file(file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pranavi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pranavi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindia_today_analyzed.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n",
      "File \u001b[0;32m<frozen os>:419\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:419\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "    \u001b[0;31m[... skipping similar frames: _walk at line 419 (7 times)]\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:419\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:368\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import glob\n",
    "from nltk.corpus import wordnet\n",
    "from copy import deepcopy\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for root, dirs, files in os.walk(\"/\"):\n",
    "    for file in files:\n",
    "        if \"india_today_analyzed.json\" in file:\n",
    "            print(os.path.join(root, file))\n",
    "\n",
    "# Configuration\n",
    "# INPUT_FILE = \"./analyzed_articles/llama3_8b/india_today_analyzed.json\"\n",
    "OUTPUT_PATH = \"/analyzed_articles/llama3_8b\"\n",
    "INPUT_FILE = glob.glob(\"./analyzed_articles/llama3_8b/india_today_.json\")\n",
    "AUGMENT_COUNT = 20  # Number of augmented versions per article\n",
    "\n",
    "def get_synonym(word):\n",
    "    \"\"\"Fetch a random synonym for a given word using WordNet.\"\"\"\n",
    "    synonyms = wordnet.synsets(word)\n",
    "    if synonyms:\n",
    "        lemmas = [lemma.name().replace('_', ' ') for s in synonyms for lemma in s.lemmas()]\n",
    "        lemmas = list(set([w for w in lemmas if w.lower() != word.lower() and w.isalpha()]))\n",
    "        if lemmas:\n",
    "            return random.choice(lemmas)\n",
    "    return word\n",
    "\n",
    "def synonym_replacement(text, replace_prob=0.3):\n",
    "    \"\"\"Randomly replace words in text with synonyms.\"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isalpha() and random.random() < replace_prob:\n",
    "            new_words.append(get_synonym(word))\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def augment_article(article, version_num):\n",
    "    \"\"\"Create an augmented version of an article.\"\"\"\n",
    "    aug = deepcopy(article)\n",
    "    fields = ['title', 'text', 'keywords', 'summary']\n",
    "    for field in fields:\n",
    "        if field in aug and isinstance(aug[field], str):\n",
    "            aug[field] = synonym_replacement(aug[field], replace_prob=0.3 + version_num * 0.01)\n",
    "        elif field in aug and isinstance(aug[field], list):\n",
    "            aug[field] = [synonym_replacement(w, replace_prob=0.4) for w in aug[field]]\n",
    "\n",
    "    if \"bias_category\" in aug and isinstance(aug[\"bias_category\"], str):\n",
    "        aug[\"bias_category\"] = synonym_replacement(aug[\"bias_category\"], replace_prob=0.5)\n",
    "\n",
    "    if \"analysis\" in aug and \"reasoning\" in aug[\"analysis\"]:\n",
    "        aug[\"analysis\"][\"reasoning\"] = synonym_replacement(aug[\"analysis\"][\"reasoning\"], replace_prob=0.3)\n",
    "\n",
    "    aug[\"augmented_version\"] = version_num\n",
    "    return aug\n",
    "\n",
    "def load_articles_from_file():\n",
    "    \"\"\"Load articles from a JSON file.\"\"\"\n",
    "    if os.path.exists(INPUT_FILE):\n",
    "        with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding {INPUT_FILE}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {INPUT_FILE}\")\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    articles = load_articles_from_file()\n",
    "    if not articles:\n",
    "        print(\"No input data found.\")\n",
    "        return\n",
    "\n",
    "    output_articles = []\n",
    "\n",
    "    for idx, article in enumerate(articles):\n",
    "        output_articles.append(article)  # include original\n",
    "        for v in range(1, AUGMENT_COUNT + 1):\n",
    "            aug = augment_article(article, version_num=v)\n",
    "            output_articles.append(aug)\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed {idx}/{len(articles)} articles\")\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(output_articles)} articles to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 10:40:13.898825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lru'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattack_recipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     PWWSRen2019,\n\u001b[1;32m      5\u001b[0m     TextFoolerJin2019,\n\u001b[1;32m      6\u001b[0m     DeepWordBugGao2018,\n\u001b[1;32m      7\u001b[0m     BAEGarg2019,\n\u001b[1;32m      8\u001b[0m     CheckList2020\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelWrapper\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Welcome to the API references for TextAttack!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mWhat is TextAttack?\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mTextAttack provides components for common NLP tasks like sentence encoding, grammar-checking, and word replacement that can be used on their own.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattack_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttackArgs, CommandLineAttackArgs\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugment_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AugmenterArgs\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetArgs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/attack_args.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARGS_SPLIT_TOKEN, load_module_from_file\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Attack\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetArgs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/shared/__init__.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validators\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattacked_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttackedText\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword_embeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractWordEmbedding, WordEmbedding, GensimWordEmbedding\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/shared/validators.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoal_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     InputReduction,\n\u001b[1;32m     13\u001b[0m     MinimizeBleu,\n\u001b[1;32m     14\u001b[0m     NonOverlappingOutput,\n\u001b[1;32m     15\u001b[0m     TargetedClassification,\n\u001b[1;32m     16\u001b[0m     UntargetedClassification,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# A list of goal functions and the corresponding available models.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/goal_functions/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\".. _goal_functions:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mGoal Functions\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mGoal Functions determine if an attack has been successful.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoal_function\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoalFunction\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/textattack/goal_functions/goal_function.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\".. _goal_function:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mGoalFunction Class\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m===========================================================\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlru\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lru'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from textattack.attack_recipes import (\n",
    "    PWWSRen2019,\n",
    "    TextFoolerJin2019,\n",
    "    DeepWordBugGao2018,\n",
    "    BAEGarg2019,\n",
    "    CheckList2020\n",
    ")\n",
    "from textattack.datasets import Dataset\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "\n",
    "\n",
    "# Dummy wrapper for compatibility\n",
    "class DummyWrapper(ModelWrapper):\n",
    "    def __init__(self):\n",
    "        self.model = lambda x: [[0.5, 0.5]] * len(x)  # Mock prediction\n",
    "\n",
    "    def __call__(self, text_input_list):\n",
    "        return self.model(text_input_list)\n",
    "\n",
    "    def get_grad(self, text_input_list, labels):\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load input file\n",
    "with open(\"/analyzed_data/llama3_8b/india_today_analyzed.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Select top N entries to attack for speed\n",
    "top_n = 10  # you can change this\n",
    "data = data[:top_n]\n",
    "\n",
    "# Initialize model\n",
    "model_wrapper = DummyWrapper()\n",
    "\n",
    "# Define attacks\n",
    "attack_recipes = {\n",
    "    \"PWWS\": PWWSRen2019.build(model_wrapper),\n",
    "    \"TextFooler\": TextFoolerJin2019.build(model_wrapper),\n",
    "    \"DeepWordBug\": DeepWordBugGao2018.build(model_wrapper),\n",
    "    \"BAE\": BAEGarg2019.build(model_wrapper),\n",
    "    \"CheckList\": CheckList2020.build(model_wrapper),\n",
    "}\n",
    "\n",
    "# Prepare dataset for attack\n",
    "augmented_data = []\n",
    "\n",
    "print(f\"Starting attacks on {len(data)} articles...\")\n",
    "\n",
    "for item in tqdm(data):\n",
    "    fields_to_augment = [\"source\", \"title\", \"text\", \"keywords\"]\n",
    "    result_entry = {\n",
    "        \"original\": item,\n",
    "        \"augmented_versions\": {}\n",
    "    }\n",
    "\n",
    "    for name, attack in attack_recipes.items():\n",
    "        field_augmentations = {}\n",
    "        for field in fields_to_augment:\n",
    "            original_field_value = item.get(field, \"\")\n",
    "            # If keywords is a list, join to a string\n",
    "            if isinstance(original_field_value, list):\n",
    "                original_field_value = \", \".join(original_field_value)\n",
    "            try:\n",
    "                result = attack.attack(original_field_value, \"\")\n",
    "                if isinstance(result, SuccessfulAttackResult):\n",
    "                    adv_text = result.perturbed_text()\n",
    "                else:\n",
    "                    adv_text = result.perturbed_text()\n",
    "                field_augmentations[field] = adv_text\n",
    "            except Exception as e:\n",
    "                field_augmentations[field] = f\"Error: {str(e)}\"\n",
    "        result_entry[\"augmented_versions\"][name] = field_augmentations\n",
    "\n",
    "    augmented_data.append(result_entry)\n",
    "\n",
    "\n",
    "for item in tqdm(data):\n",
    "    original_text = item.get(\"text\", \"\")\n",
    "    result_entry = {\n",
    "        \"original\": item,\n",
    "        \"augmented_versions\": {}\n",
    "    }\n",
    "\n",
    "    for name, attack in attack_recipes.items():\n",
    "        try:\n",
    "            result = attack.attack(original_text, \"\")\n",
    "            if isinstance(result, SuccessfulAttackResult):\n",
    "                adv_text = result.perturbed_text()\n",
    "            else:\n",
    "                adv_text = result.perturbed_text()\n",
    "            result_entry[\"augmented_versions\"][name] = adv_text\n",
    "        except Exception as e:\n",
    "            result_entry[\"augmented_versions\"][name] = f\"Error: {str(e)}\"\n",
    "\n",
    "    augmented_data.append(result_entry)\n",
    "\n",
    "# Save output\n",
    "with open(\"augmented_india_today.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(augmented_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Done! Saved to 'augmented_data_india_today.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /opt/anaconda3/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: lemminflect in /opt/anaconda3/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from lemminflect) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba lemminflect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Using cached flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting boto3>=1.20.27 (from flair)\n",
      "  Using cached boto3-1.37.37-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
      "  Using cached conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown>=4.4.0 (from flair)\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (0.30.2)\n",
      "Collecting langdetect>=1.0.9 (from flair)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (3.8.0)\n",
      "Requirement already satisfied: more-itertools>=8.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (10.1.0)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Using cached mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair)\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (2.8.2)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (2023.10.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (1.6.1)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: tabulate>=0.8.10 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in /opt/anaconda3/lib/python3.11/site-packages (from flair) (4.65.0)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Using cached transformer_smaller_training_vocab-0.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.51.3)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Using cached Wikipedia_API-0.8.1-py3-none-any.whl\n",
      "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
      "  Using cached bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: intervaltree in /opt/anaconda3/lib/python3.11/site-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
      "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting botocore<1.38.0,>=1.37.37 (from boto3>=1.20.27->flair)\n",
      "  Using cached botocore-1.37.37-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.20.27->flair)\n",
      "  Using cached s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.11/site-packages (from ftfy>=6.1.0->flair) (0.2.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (from gdown>=4.4.0->flair) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from gdown>=4.4.0->flair) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/lib/python3.11/site-packages (from gdown>=4.4.0->flair) (2.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.10.0->flair) (4.9.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=2.2.3->flair) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from mpld3>=0.3->flair) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.13.1->flair) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch>=1.13.1->flair) (3.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.11/site-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (3.20.3)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/anaconda3/lib/python3.11/site-packages (from botocore<1.38.0,>=1.37.37->boto3>=1.20.27->flair) (2.0.7)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
      "Collecting accelerate>=0.26.0 (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair)\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->mpld3>=0.3->flair) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.1->flair) (1.3.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.0)\n",
      "Using cached flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
      "Using cached bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Using cached boto3-1.37.37-py3-none-any.whl (139 kB)\n",
      "Using cached conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Using cached transformer_smaller_training_vocab-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached botocore-1.37.37-py3-none-any.whl (13.5 MB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Installing collected packages: sqlitedict, sentencepiece, pptree, docopt, segtok, langdetect, jsonlines, ftfy, deprecated, conllu, wikipedia-api, botocore, bioc, s3transfer, pytorch-revgrad, mpld3, gdown, accelerate, boto3, transformer-smaller-training-vocab, flair\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "textattack 0.3.10 requires bert-score>=0.3.5, which is not installed.\n",
      "textattack 0.3.10 requires datasets>=2.4.0, which is not installed.\n",
      "textattack 0.3.10 requires editdistance, which is not installed.\n",
      "textattack 0.3.10 requires jieba, which is not installed.\n",
      "textattack 0.3.10 requires language-tool-python, which is not installed.\n",
      "textattack 0.3.10 requires lemminflect, which is not installed.\n",
      "textattack 0.3.10 requires lru-dict, which is not installed.\n",
      "textattack 0.3.10 requires num2words, which is not installed.\n",
      "textattack 0.3.10 requires OpenHowNet, which is not installed.\n",
      "textattack 0.3.10 requires pinyin>=0.4.0, which is not installed.\n",
      "textattack 0.3.10 requires terminaltables, which is not installed.\n",
      "textattack 0.3.10 requires word2number, which is not installed.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.37.37 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.6.0 bioc-2.1 boto3-1.37.37 botocore-1.37.37 conllu-4.5.3 deprecated-1.2.18 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 gdown-5.2.0 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.11.5 segtok-1.5.11 sentencepiece-0.2.0 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.1 wikipedia-api-0.8.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "!pip install flair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
